{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from model import MLPNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('home-data-for-ml-course/train.csv')\n",
    "test_df = pd.read_csv('home-data-for-ml-course/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1460, 81)\n",
      "Test dataset shape: (1459, 80)\n",
      "Number of columns in training dataset: 81\n",
      "Number of columns in test dataset: 80\n",
      "\n",
      "Missing values in training dataset:\n",
      "PoolQC         1453\n",
      "MiscFeature    1406\n",
      "Alley          1369\n",
      "Fence          1179\n",
      "MasVnrType      872\n",
      "FireplaceQu     690\n",
      "LotFrontage     259\n",
      "GarageYrBlt      81\n",
      "GarageCond       81\n",
      "GarageType       81\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test dataset:\n",
      "PoolQC          1456\n",
      "MiscFeature     1408\n",
      "Alley           1352\n",
      "Fence           1169\n",
      "MasVnrType       894\n",
      "FireplaceQu      730\n",
      "LotFrontage      227\n",
      "GarageYrBlt       78\n",
      "GarageQual        78\n",
      "GarageFinish      78\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the training and test datasets\n",
    "print(\"Training dataset shape:\", train_df.shape)\n",
    "print(\"Test dataset shape:\", test_df.shape)\n",
    "\n",
    "# Print the number of columns in each dataset\n",
    "print(f\"Number of columns in training dataset: {train_df.shape[1]}\")\n",
    "print(f\"Number of columns in test dataset: {test_df.shape[1]}\")\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training dataset:\")\n",
    "print(train_df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nMissing values in test dataset:\")\n",
    "print(test_df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 0 is numerical, 1 is categorical\n",
    "columns_dtypes = {\n",
    "    'MSSubClass': 1,\n",
    "    'MSZoning': 1,\n",
    "    'LotFrontage': 0,\n",
    "    'LotArea': 0,\n",
    "    'Street': 1,\n",
    "    'Alley': 1,\n",
    "    'LotShape': 1,\n",
    "    'LandContour': 1,\n",
    "    'Utilities': 1,\n",
    "    'LotConfig': 1,\n",
    "    'LandSlope': 1,\n",
    "    'Neighborhood': 1,\n",
    "    'Condition1': 1,\n",
    "    'Condition2': 1,\n",
    "    'BldgType': 1,\n",
    "    'HouseStyle': 1,\n",
    "    'OverallQual': 1,\n",
    "    'OverallCond': 1,\n",
    "    'YearBuilt': 0,\n",
    "    'YearRemodAdd': 0,\n",
    "    'RoofStyle': 1,\n",
    "    'RoofMatl': 1,\n",
    "    'Exterior1st': 1,\n",
    "    'Exterior2nd': 1,\n",
    "    'MasVnrType': 1,\n",
    "    'MasVnrArea': 0,\n",
    "    'ExterQual': 1,\n",
    "    'ExterCond': 1,\n",
    "    'Foundation': 1,\n",
    "    'BsmtQual': 1,\n",
    "    'BsmtCond': 1,\n",
    "    'BsmtExposure': 1,\n",
    "    'BsmtFinType1': 1,\n",
    "    'BsmtFinSF1': 0,\n",
    "    'BsmtFinType2': 1,\n",
    "    'BsmtFinSF2': 0,\n",
    "    'BsmtUnfSF': 0,\n",
    "    'TotalBsmtSF': 0,\n",
    "    'Heating': 1,\n",
    "    'HeatingQC': 1,\n",
    "    'CentralAir': 1,\n",
    "    'Electrical': 1,\n",
    "    '1stFlrSF': 0,\n",
    "    '2ndFlrSF': 0,\n",
    "    'LowQualFinSF': 0,\n",
    "    'GrLivArea': 0,\n",
    "    'BsmtFullBath': 0,\n",
    "    'BsmtHalfBath': 0,\n",
    "    'FullBath': 0,\n",
    "    'HalfBath': 0,\n",
    "    'BedroomAbvGr': 0,\n",
    "    'KitchenAbvGr': 0,\n",
    "    'KitchenQual': 1,\n",
    "    'TotRmsAbvGrd': 0,\n",
    "    'Functional': 1,\n",
    "    'Fireplaces': 0,\n",
    "    'FireplaceQu': 1,\n",
    "    'GarageType': 1,\n",
    "    'GarageYrBlt': 0,\n",
    "    'GarageFinish': 1,\n",
    "    'GarageCars': 0,\n",
    "    'GarageArea': 0,\n",
    "    'GarageQual': 1,\n",
    "    'GarageCond': 1,\n",
    "    'PavedDrive': 1,\n",
    "    'WoodDeckSF': 0,\n",
    "    'OpenPorchSF': 0,\n",
    "    'EnclosedPorch': 0,\n",
    "    '3SsnPorch': 0,\n",
    "    'ScreenPorch': 0,\n",
    "    'PoolArea': 0,\n",
    "    'PoolQC': 1,\n",
    "    'Fence': 1,\n",
    "    'MiscFeature': 1,\n",
    "    'MiscVal': 0,\n",
    "    'MoSold': 0,\n",
    "    'YrSold': 0,\n",
    "    'SaleType': 1,\n",
    "    'SaleCondition': 1,\n",
    "    'SalePrice': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def nomalization(data: pd.DataFrame, label_col: str):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data_to_normalize = data.copy()\n",
    "    \n",
    "    # Only drop the label column if it exists and is specified\n",
    "    if label_col and label_col in data.columns:\n",
    "        data_to_normalize = data.drop(columns=[label_col])\n",
    "        \n",
    "    features_max = data_to_normalize.max()\n",
    "    features_min = data_to_normalize.min()\n",
    "    \n",
    "    # Handle division by zero (when max == min)\n",
    "    range_values = features_max - features_min\n",
    "    range_values = range_values.replace(0, 1)  # Replace zeros with 1 to avoid division by zero\n",
    "    \n",
    "    normalized_data = 2 * (data_to_normalize - features_min) / range_values - 1\n",
    "    \n",
    "    # Add back the label column if it exists\n",
    "    if label_col and label_col in data.columns:\n",
    "        normalized_data[label_col] = data[label_col]\n",
    "        \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame, columns_dtype: dict, label_col: str, fit_encoders=False):\n",
    "    imputer = KNNImputer(n_neighbors=int(np.sqrt(len(data))))\n",
    "    for col in data.columns:\n",
    "        if col in columns_dtype and columns_dtype[col] == 1: # categorical\n",
    "            data[col] = data[col].fillna('unknown')\n",
    "        elif col in columns_dtype and columns_dtype[col] == 0: # numerical\n",
    "            imputed_values = imputer.fit_transform(data[[col]])\n",
    "            data[col] = imputed_values.flatten()\n",
    "                \n",
    "    categorical_cols = [col for col in data.columns if col in columns_dtype and columns_dtype[col] == 1]\n",
    "    numerical_cols = [col for col in data.columns if col in columns_dtype and columns_dtype[col] == 0]\n",
    "    \n",
    "    # For train data, fit and transform. For test data, just transform\n",
    "    if fit_encoders:\n",
    "        global encoder\n",
    "        encoded_categorical_data = encoder.fit_transform(data[categorical_cols])\n",
    "    else:\n",
    "        encoded_categorical_data = encoder.transform(data[categorical_cols])\n",
    "        \n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    for feature in encoded_feature_names:\n",
    "        if feature not in columns_dtype:\n",
    "            columns_dtype[feature] = 1\n",
    "    \n",
    "    categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoded_feature_names, index=data.index)\n",
    "    \n",
    "    normalized_numerical_data = nomalization(data[numerical_cols], label_col)\n",
    "    \n",
    "    df = pd.concat([categorical_df, normalized_numerical_data], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_np(df: pd.DataFrame, label_col: str):\n",
    "    if label_col:\n",
    "        features_data = df.drop(columns=[label_col]).to_numpy()\n",
    "        label_data = df[label_col].to_numpy()\n",
    "        return features_data, label_data\n",
    "    else:\n",
    "        return df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(features: np.ndarray, labels: np.ndarray, k: int):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    return kf.split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store encoders to ensure consistency between train and test processing\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
    "# Store min/max values for consistent normalization\n",
    "numerical_min_max = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transformer = FunctionTransformer(func=preprocess_data, kw_args={'columns_dtype': columns_dtypes, 'label_col': 'SalePrice', 'fit_encoders': True})\n",
    "np_transformer = FunctionTransformer(func=df_to_np, kw_args={'label_col': 'SalePrice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessed\n",
      "Dataframe turned into numpy arrays\n",
      "Training features shape:  (1460, 288)\n",
      "Training labels shape:  (1460,)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocess_transformer),\n",
    "    ('df_to_np', np_transformer)\n",
    "])\n",
    "\n",
    "train_features, train_labels = pipeline.fit_transform(train_df.drop(columns=['Id']))\n",
    "\n",
    "print('Data preprocessed')\n",
    "print('Dataframe turned into numpy arrays')\n",
    "print('Training features shape: ', train_features.shape)\n",
    "print('Training labels shape: ', train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_transformer = FunctionTransformer(func=preprocess_data, kw_args={'columns_dtype': columns_dtypes, 'label_col': None, 'fit_encoders': False})\n",
    "test_np_transformer = FunctionTransformer(func=df_to_np, kw_args={'label_col': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data preprocessed\n",
      "Dataframe turned into numpy arrays\n",
      "Testing features shape:  (1459, 288)\n",
      "Testing ids shape:  (1459,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lazy_debt/Desktop/new/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 6, 18, 19, 33, 34, 44] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_data_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', test_preprocess_transformer),\n",
    "    ('df_to_np', test_np_transformer)\n",
    "])\n",
    "\n",
    "test_features = test_data_pipeline.fit_transform(test_df.drop(columns=['Id']))\n",
    "test_id = df_to_np(test_df['Id'], None)\n",
    "\n",
    "print('Testing data preprocessed')\n",
    "print('Dataframe turned into numpy arrays')\n",
    "print('Testing features shape: ', test_features.shape)\n",
    "print('Testing ids shape: ', test_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_index = []\n",
    "for train_index, test_index in k_fold(train_features, train_labels, k=7):\n",
    "    k_fold_index.append([train_index, test_index])\n",
    "\n",
    "params = {\n",
    "    'alphas': [0.6, 0.7, 0.8, 0.9],\n",
    "    'lambdas': [0, 0.1, 0.2, 0.3],\n",
    "    'epsilons': [math.pow(math.e, -7), math.pow(math.e, -6), math.pow(math.e, -5)],\n",
    "    'hidden_sizes': [5, 10, 20],\n",
    "    'neurons_per_layer': [5, 20, 45],\n",
    "    'early_stopping_threshold': 150000,\n",
    "    'early_stopping_folds': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters successfully loaded from best_params.pkl\n",
      "Loaded parameters: {'alpha': 0.9, 'lambda': 0.2, 'epsilon': 0.006737946999085469, 'layer': 5, 'npl': 20, 'mean_mae': 160475.4781737818, 'folds_completed': 3, 'input_size': 288, 'output_size': 1, 'timestamp': '2025-03-28T21:45:48.634819'}\n",
      "Automatically loaded parameters from best_params.pkl\n",
      "Model: MLP - Multi-Layer Perceptron\n",
      "Type: Regression\n",
      "Purpose: Housing Prices Prediction\n",
      "\n",
      "Architecture:\n",
      "  Input size: 288\n",
      "  Output size: 1\n",
      "  Hidden layers: 5\n",
      "  Neurons per layer: 20\n",
      "\n",
      "Hyperparameters:\n",
      "  Learning rate (alpha): 0.9\n",
      "  Regularization (lambda): 0.2\n",
      "  Convergence threshold (epsilon): 0.006737946999085469\n",
      "\n",
      "Performance: MAE = 160475.478174\n",
      "\n",
      "Last trained: 2025-03-28T21:45:48.634819\n",
      "\n",
      "Status: Model needs initialization before training/prediction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_network = MLPNeuralNetwork(len(train_features[0]), 1)\n",
    "neural_network.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping enabled: Will skip parameter sets with average RMSE > 150000 after 4 folds\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.0361723129\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.37314478235\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.1680173917\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.06770380505\n",
      "\n",
      "Early stopping: Current RMSE (187961.4113) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.41125957298\n",
      "\n",
      "New best parameters found with RMSE: 187961.4113\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.9797534927\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.14092741703\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.4267223205\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.05628372062\n",
      "\n",
      "Early stopping: Current RMSE (176822.4009) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.40092173772\n",
      "\n",
      "New best parameters found with RMSE: 176822.4009\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.02974881598\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.30446563137\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.1385245155\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.0348152841\n",
      "\n",
      "Early stopping: Current RMSE (187959.3769) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.37688856173\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.4713903946\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.59257920095\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175785.92110457327\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.56075737628\n",
      "\n",
      "Early stopping: Current RMSE (176820.8865) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.88645788628\n",
      "\n",
      "New best parameters found with RMSE: 176820.8865\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.34185080498\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.68830782254\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.47721047542\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.37741422438\n",
      "\n",
      "Early stopping: Current RMSE (187961.7212) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.72119583184\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.70171314932\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.90853391524\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.1455483418\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.763589875\n",
      "\n",
      "Early stopping: Current RMSE (176824.1298) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.12984632034\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.9021447833\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.20393058666\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.02095112132\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.91871830833\n",
      "\n",
      "Early stopping: Current RMSE (187960.2614) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.26143619991\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.38406977017\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.58247027517\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.8284829731\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.44864948755\n",
      "\n",
      "Early stopping: Current RMSE (176823.8109) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.81091812652\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.24060101015\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.5839164665\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.37479654423\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.27482893597\n",
      "\n",
      "Early stopping: Current RMSE (187961.6185) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.61853573922\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.07404806375\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.2377216248\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.52084528084\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.14977583717\n",
      "\n",
      "Early stopping: Current RMSE (176822.4956) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.49559770167\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.4156677874\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.7333872533\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.54037848566\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.43901475394\n",
      "\n",
      "Early stopping: Current RMSE (187960.7821) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.78211207007\n",
      "\n",
      "Training with alpha=0.6, lambda=0, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.54977654814\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.75256969887\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.99388821432\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.61294617556\n",
      "\n",
      "Early stopping: Current RMSE (176823.9773) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.97729515925\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.55082741237\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.84171245148\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.66559438084\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.5627670064\n",
      "\n",
      "Early stopping: Current RMSE (187959.9052) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.90522531274\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.49911973978\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.6475526155\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.9469633235\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.5797402595\n",
      "\n",
      "Early stopping: Current RMSE (176821.9183) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.91834398458\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.7766327367\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.07452412517\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.8939959673\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.7915507406\n",
      "\n",
      "Early stopping: Current RMSE (187960.1342) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.13417589245\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.86923129766\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.0539841537\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.31458138616\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.93819212765\n",
      "\n",
      "Early stopping: Current RMSE (176823.2940) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.29399724127\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.34241472618\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.59580456844\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.44328755382\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.33841511235\n",
      "\n",
      "Early stopping: Current RMSE (187958.6800) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.6799804902\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.91329546255\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.09921640012\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.3585653631\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.98188131783\n",
      "\n",
      "Early stopping: Current RMSE (176823.3382) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.3382396359\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.02565554687\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.2692167702\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.1228863076\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.0174778022\n",
      "\n",
      "Early stopping: Current RMSE (187958.3588) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.35880910672\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.63251133836\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.83749766837\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.07647245735\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.6949769377\n",
      "\n",
      "Early stopping: Current RMSE (176824.0604) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.06036460045\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.37161294481\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.68796547502\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.49581711422\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.3943788249\n",
      "\n",
      "Early stopping: Current RMSE (187960.7374) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.73744358972\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.8738611805\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.05873676852\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.3192028436\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.9427826121\n",
      "\n",
      "Early stopping: Current RMSE (176823.2986) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.2986458512\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.9136930799\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.18480886536\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.0211343909\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.9172287522\n",
      "\n",
      "Early stopping: Current RMSE (187959.2592) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.25921627207\n",
      "\n",
      "Training with alpha=0.6, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.38835688494\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.5603623872\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.83458209035\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.46140989114\n",
      "\n",
      "Early stopping: Current RMSE (176822.8112) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.81117781342\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.6973007213\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.96170217535\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.80225398554\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.69798213473\n",
      "\n",
      "Early stopping: Current RMSE (187959.0398) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.03980975423\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.54412025487\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.74676346875\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.98824221946\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.60733802247\n",
      "\n",
      "Early stopping: Current RMSE (176823.9716) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.97161599138\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.9158485916\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.21805960147\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.03481249613\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.93260287633\n",
      "\n",
      "Early stopping: Current RMSE (187960.2753) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.2753308914\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.52974343929\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.67898811508\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.9775312866\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.61010334358\n",
      "\n",
      "Early stopping: Current RMSE (176821.9491) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.94909154612\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.1872070517\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.49783781733\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.3092909708\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.20754059457\n",
      "\n",
      "Early stopping: Current RMSE (187960.5505) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.55046910857\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.73491710873\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.88960070908\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.1823315388\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.81353093794\n",
      "\n",
      "Early stopping: Current RMSE (176822.1551) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.15509507363\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.43101893604\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.68715804577\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.53291052612\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.42818803384\n",
      "\n",
      "Early stopping: Current RMSE (187958.7698) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.76981888546\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.2974254239\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.4935291105\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.7419962983\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.36274245396\n",
      "\n",
      "Early stopping: Current RMSE (176823.7239) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.72392332167\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.69144883327\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.98669711468\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.80783263897\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.7052432493\n",
      "\n",
      "Early stopping: Current RMSE (187960.0478) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.04780545906\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.7077788583\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.86174305464\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.15524268002\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.7866236406\n",
      "\n",
      "Early stopping: Current RMSE (176822.1278) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.12784705838\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.9324729116\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.23519974426\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.05162795878\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.94944647368\n",
      "\n",
      "Early stopping: Current RMSE (187960.2922) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.2921867721\n",
      "\n",
      "Training with alpha=0.6, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.87815060376\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.06313989908\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.32348446237\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.94703553477\n",
      "\n",
      "Early stopping: Current RMSE (176823.3030) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.302952625\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.44260093736\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.6990994208\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.54462569684\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.43992280573\n",
      "\n",
      "Early stopping: Current RMSE (187958.7816) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.78156221518\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.8503887022\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.03464206913\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.2957730837\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.91950988446\n",
      "\n",
      "Early stopping: Current RMSE (176823.2751) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.27507843488\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.99708580828\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.30181746237\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.1169837602\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.01491162545\n",
      "\n",
      "Early stopping: Current RMSE (187960.3577) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.35769966405\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.5091725295\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.6313629094\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175785.95881794236\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.59821796912\n",
      "\n",
      "Early stopping: Current RMSE (176820.9244) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.92439283757\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.29576579024\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.64079288975\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.43059559213\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.33072134733\n",
      "\n",
      "Early stopping: Current RMSE (187961.6745) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.67446890488\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.92349065104\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.1096818511\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.36874200226\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.99198975362\n",
      "\n",
      "Early stopping: Current RMSE (176823.3485) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.3484760645\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.3266168505\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.61054501028\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.43880589143\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.33559907047\n",
      "\n",
      "Early stopping: Current RMSE (187959.6779) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.67789170565\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.83799216035\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.04842548055\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.28157937588\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.89870923458\n",
      "\n",
      "Early stopping: Current RMSE (176824.2667) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.26667656284\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.6562740583\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.95043092765\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.77225343278\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.66960451458\n",
      "\n",
      "Early stopping: Current RMSE (187960.0121) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.01214073334\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.0748840251\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.23857974983\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.52167972617\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.15060469307\n",
      "\n",
      "Early stopping: Current RMSE (176822.4964) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.49643704854\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.36030280186\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.61424768495\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.46138130396\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.35653913618\n",
      "\n",
      "Early stopping: Current RMSE (187958.6981) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.69811773172\n",
      "\n",
      "Training with alpha=0.6, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193204.43133422898\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173476.6840043985\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175790.87202236208\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164831.4784933788\n",
      "\n",
      "Early stopping: Current RMSE (176825.8665) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176825.8664635921\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.70372337083\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.0303806337\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.8317460523\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.73086982613\n",
      "\n",
      "Early stopping: Current RMSE (187961.0742) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.07417997075\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.47111766826\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.67182568755\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.91537247016\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.53495664668\n",
      "\n",
      "Early stopping: Current RMSE (176823.8983) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.89831811818\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.6993272652\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.96379159822\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.80430382746\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.70003540523\n",
      "\n",
      "Early stopping: Current RMSE (187959.0419) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.04186452404\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.0507698103\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.26684353477\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.49396984803\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.1096762738\n",
      "\n",
      "Early stopping: Current RMSE (176824.4803) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.48031486673\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.0458935286\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.38316762715\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.17785037914\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.07755324486\n",
      "\n",
      "Early stopping: Current RMSE (187961.4211) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.42111619495\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.08401783535\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.24795568312\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.53079690816\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.1596607655\n",
      "\n",
      "Early stopping: Current RMSE (176822.5056) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.50560779806\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.92367159744\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.19509700013\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.03122763816\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.92733888605\n",
      "\n",
      "Early stopping: Current RMSE (187959.2693) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.26933378045\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.96060193007\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.12126816984\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.40760561376\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.0372951408\n",
      "\n",
      "Early stopping: Current RMSE (176822.3817) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.3816927136\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.71639706206\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.98139104582\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.82156989045\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.7173303563\n",
      "\n",
      "Early stopping: Current RMSE (187959.0592) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.0591720887\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.79142317522\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.94760468375\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.23873475965\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.86955612124\n",
      "\n",
      "Early stopping: Current RMSE (176822.2118) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.21182968497\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.09858938056\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.40647050258\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.2196543924\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.11775403947\n",
      "\n",
      "Early stopping: Current RMSE (187960.4606) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.46061707873\n",
      "\n",
      "Training with alpha=0.7, lambda=0, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.1205376064\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.311952247\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.56543036932\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.18735988546\n",
      "\n",
      "Early stopping: Current RMSE (176823.5463) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.54632002703\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.68721333984\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.01335833073\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.8150461957\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.71414202848\n",
      "\n",
      "Early stopping: Current RMSE (187961.0574) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.0574399737\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.0396594983\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.25543870754\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.48287975392\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.09866050695\n",
      "\n",
      "Early stopping: Current RMSE (176824.4692) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.46915961668\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.51904774836\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.80894671843\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.63344932083\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.53056816335\n",
      "\n",
      "Early stopping: Current RMSE (187959.8730) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.87300298773\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.01835984708\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.1275392131\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175785.46889866033\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.11158246486\n",
      "\n",
      "Early stopping: Current RMSE (176820.4316) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.43159504634\n",
      "\n",
      "New best parameters found with RMSE: 176820.4316\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.36195821952\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.70903912754\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.49754908052\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.3977868603\n",
      "\n",
      "Early stopping: Current RMSE (187961.7416) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.741583322\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.71232198377\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.91942397525\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.15613787438\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.77410843715\n",
      "\n",
      "Early stopping: Current RMSE (176824.1405) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.14049806763\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.78039887612\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182565.14046305648\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.92080081918\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.82174677946\n",
      "\n",
      "Early stopping: Current RMSE (187962.1659) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187962.1658523828\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.63241010054\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.78437635422\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.08001109172\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.7118962842\n",
      "\n",
      "Early stopping: Current RMSE (176822.0522) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.0521734577\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.29682635603\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.5798301644\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.40867286935\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.30541563092\n",
      "\n",
      "Early stopping: Current RMSE (187959.6477) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.64768625516\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193198.95890066845\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.0665038211\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175785.40954771522\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.05262933436\n",
      "\n",
      "Early stopping: Current RMSE (176820.3719) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.3718953848\n",
      "\n",
      "New best parameters found with RMSE: 176820.3719\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.3573019705\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.67321045883\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.4813415963\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.37987908718\n",
      "\n",
      "Early stopping: Current RMSE (187960.7229) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.72293327822\n",
      "\n",
      "Training with alpha=0.7, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.3792590474\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.57753202945\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.82368100845\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.4438797072\n",
      "\n",
      "Early stopping: Current RMSE (176823.8061) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.8060879481\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.86638918804\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182565.22912144908\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189110.00777981232\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.90887130602\n",
      "\n",
      "Early stopping: Current RMSE (187962.2530) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187962.25304043887\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.03916076466\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.17540061587\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.48784150954\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.1236957056\n",
      "\n",
      "Early stopping: Current RMSE (176821.4565) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.45652464894\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.61947299156\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.88145956417\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.72353140314\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.6191278406\n",
      "\n",
      "Early stopping: Current RMSE (187958.9609) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.96089794987\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.65853180754\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.86420790275\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.10244558015\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.72077598923\n",
      "\n",
      "Early stopping: Current RMSE (176824.0865) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.08649031993\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.33168625637\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.64679993875\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.4554313606\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.35392549998\n",
      "\n",
      "Early stopping: Current RMSE (187960.6970) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.69696076392\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.26323268868\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.43192130496\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.7096856072\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.3373504998\n",
      "\n",
      "Early stopping: Current RMSE (176822.6855) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.68554752518\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.11989482903\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.39740873614\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.22970701582\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.1261503452\n",
      "\n",
      "Early stopping: Current RMSE (187959.4683) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.46829023154\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.43665428838\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.58343127475\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.88461156428\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.5178064097\n",
      "\n",
      "Early stopping: Current RMSE (176821.8556) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.8556258843\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.08987055565\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.3974811505\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.2108353237\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.10892021624\n",
      "\n",
      "Early stopping: Current RMSE (187960.4518) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.4517768115\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.97732630267\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.19145315583\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.42065998315\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.03685772893\n",
      "\n",
      "Early stopping: Current RMSE (176824.4066) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.40657429266\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.35230549434\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.63703073017\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.4647898957\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.3616265489\n",
      "\n",
      "Early stopping: Current RMSE (187959.7039) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.70393816728\n",
      "\n",
      "Training with alpha=0.7, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.99703166375\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.13215471475\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.44578909015\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.08192514512\n",
      "\n",
      "Early stopping: Current RMSE (176821.4142) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.41422515345\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.35620772527\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.6720822623\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.48023477185\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.37877041168\n",
      "\n",
      "Early stopping: Current RMSE (187960.7218) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.7218237928\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.97648635125\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.19059094004\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.41982156254\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.03602492844\n",
      "\n",
      "Early stopping: Current RMSE (176824.4057) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.40573094555\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.51024820487\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.83090231672\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.6360463647\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.53484270227\n",
      "\n",
      "Early stopping: Current RMSE (187960.8780) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.87800989713\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193198.66470542623\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173470.76450969852\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175785.1158880057\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164825.76093796548\n",
      "\n",
      "Early stopping: Current RMSE (176820.0765) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.07651027397\n",
      "\n",
      "New best parameters found with RMSE: 176820.0765\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.48711075\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.77601876983\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.6011451193\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.49820991346\n",
      "\n",
      "Early stopping: Current RMSE (187959.8406) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.84062113817\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.87194909496\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.05677399333\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.31729423697\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.94088679942\n",
      "\n",
      "Early stopping: Current RMSE (176823.2967) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.29672603117\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.5779807001\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.86970826084\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.69305987394\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.59027845395\n",
      "\n",
      "Early stopping: Current RMSE (187959.9328) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.9327568222\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.19230836982\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.41213405965\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.63525086944\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.25001044554\n",
      "\n",
      "Early stopping: Current RMSE (176824.6224) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.6224259361\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.520185216\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.77909102838\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.62310202775\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.51853043615\n",
      "\n",
      "Early stopping: Current RMSE (187958.8602) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.86022717704\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.07989529218\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.29674109106\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.5230423387\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.1385539261\n",
      "\n",
      "Early stopping: Current RMSE (176824.5096) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.509558162\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.4784506725\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.79811817184\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.60388323574\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.5026257592\n",
      "\n",
      "Early stopping: Current RMSE (187960.8458) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.84576945982\n",
      "\n",
      "Training with alpha=0.7, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193204.79095035308\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173477.05315328363\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175791.23098422182\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164831.8350496167\n",
      "\n",
      "Early stopping: Current RMSE (176826.2275) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176826.2275343688\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.16109361753\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.40885730778\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.25988163205\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.15470233306\n",
      "\n",
      "Early stopping: Current RMSE (187958.4961) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.4961337226\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.8913211624\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.10316814942\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.3348113352\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.95158443207\n",
      "\n",
      "Early stopping: Current RMSE (176824.3202) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.32022126977\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.49092826893\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.84201083134\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.62800198104\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.5284580322\n",
      "\n",
      "Early stopping: Current RMSE (187961.8723) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.87234977836\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.1473568091\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.36599090396\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.59038109836\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.20544138408\n",
      "\n",
      "Early stopping: Current RMSE (176824.5773) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.57729254887\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.32941462932\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.67548577988\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.4646313133\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.36481401505\n",
      "\n",
      "Early stopping: Current RMSE (187961.7086) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.70858643437\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.04288424697\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.23224040493\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.48791832142\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.11036733288\n",
      "\n",
      "Early stopping: Current RMSE (176823.4684) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.46835257654\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197621.77827326828\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182566.16929905204\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189110.9301483613\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182553.83278316972\n",
      "\n",
      "Early stopping: Current RMSE (187963.1776) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187963.17762596285\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.70247402525\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.88280636683\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.1481275792\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.77285392434\n",
      "\n",
      "Early stopping: Current RMSE (176823.1266) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.12656547388\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.114371454\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.4227422649\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.23561792466\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.13374428116\n",
      "\n",
      "Early stopping: Current RMSE (187960.4766) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.4766189812\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.75901012827\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.91433240462\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.20638070462\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.8374189157\n",
      "\n",
      "Early stopping: Current RMSE (176822.1793) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.1792855383\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.81561725857\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.0836898983\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.92193090686\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.81785928842\n",
      "\n",
      "Early stopping: Current RMSE (187959.1598) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.15977433804\n",
      "\n",
      "Training with alpha=0.8, lambda=0, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.5548050311\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.7312228601\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.00072731965\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.62644157297\n",
      "\n",
      "Early stopping: Current RMSE (176822.9783) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.97829919594\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.0815321952\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.3268271995\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.17940542195\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.0740914789\n",
      "\n",
      "Early stopping: Current RMSE (187958.4155) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.41546407389\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193204.36187434537\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173476.61270325736\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175790.80268884875\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164831.40962451254\n",
      "\n",
      "Early stopping: Current RMSE (176825.7967) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176825.79672274098\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.58430859097\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.90726065272\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.7109582774\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.6098799539\n",
      "\n",
      "Early stopping: Current RMSE (187960.9531) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.95310186877\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.39177168036\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.61688482636\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.834351245\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.44777647217\n",
      "\n",
      "Early stopping: Current RMSE (176824.8227) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.82269605598\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.2940827951\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.63905767692\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.42889324736\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.32901615338\n",
      "\n",
      "Early stopping: Current RMSE (187961.6728) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.6727624682\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.95718575857\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.1177614411\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.4041956623\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.03390804524\n",
      "\n",
      "Early stopping: Current RMSE (176822.3783) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.3782627268\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.89347327437\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.19499001786\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.01217991178\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.9099324239\n",
      "\n",
      "Early stopping: Current RMSE (187960.2526) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.25264390698\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.25555648925\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.45055029294\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.7002035545\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.32122980847\n",
      "\n",
      "Early stopping: Current RMSE (176823.6819) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.6818850363\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.1651071086\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.47505215378\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.2869369272\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.18514914883\n",
      "\n",
      "Early stopping: Current RMSE (187960.5281) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.5280613346\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.6031702701\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.80737881217\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.0471847808\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.66588554851\n",
      "\n",
      "Early stopping: Current RMSE (176824.0309) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.0309048529\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197615.2222410688\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182559.40984419966\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189104.29873620239\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182547.19027575984\n",
      "\n",
      "Early stopping: Current RMSE (187956.5303) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187956.5302743077\n",
      "\n",
      "Training with alpha=0.8, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.1828166428\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.37588221562\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.62759607582\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.2491089519\n",
      "\n",
      "Early stopping: Current RMSE (176823.6089) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.60885097153\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.41098426626\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.69753020784\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.5241433471\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.4210793065\n",
      "\n",
      "Early stopping: Current RMSE (187959.7634) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.76343428195\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.69739982387\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.90410625108\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.14124286745\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.75931325814\n",
      "\n",
      "Early stopping: Current RMSE (176824.1255) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.12551555014\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.94648109\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.2496425713\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.06579720014\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.96363942232\n",
      "\n",
      "Early stopping: Current RMSE (187960.3064) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.30639007094\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193205.37659377162\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173477.65432098563\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175791.81556221042\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164832.41571004764\n",
      "\n",
      "Early stopping: Current RMSE (176826.8155) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176826.81554675382\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.65475404167\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.9798918841\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.7822136891\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.6812545881\n",
      "\n",
      "Early stopping: Current RMSE (187961.0245) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.02452855074\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.75426360627\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.90946006024\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.20164282495\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.83271279334\n",
      "\n",
      "Early stopping: Current RMSE (176822.1745) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.17451982122\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.9685234325\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.24134052082\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.07659517575\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.9727823301\n",
      "\n",
      "Early stopping: Current RMSE (187959.3148) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.3148103648\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.57035304938\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.80020009304\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175790.01260768008\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.62483826323\n",
      "\n",
      "Early stopping: Current RMSE (176825.0020) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176825.00199977143\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.6885052719\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.01469034955\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.81635298324\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.71545100296\n",
      "\n",
      "Early stopping: Current RMSE (187961.0587) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.05874990192\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.23315543018\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.42755541217\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.67784325985\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.2990193754\n",
      "\n",
      "Early stopping: Current RMSE (176823.6594) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.6593933694\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.3664558157\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.68264833317\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.49060069333\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.38915367657\n",
      "\n",
      "Early stopping: Current RMSE (187960.7322) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.7322146297\n",
      "\n",
      "Training with alpha=0.8, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.69730413804\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.8509906616\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.144787024\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.77623806262\n",
      "\n",
      "Early stopping: Current RMSE (176822.1173) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.11732997157\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.1473938842\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.456789322\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.2690200426\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.16720228744\n",
      "\n",
      "Early stopping: Current RMSE (187960.5101) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.51010138408\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.86247739068\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.02054246768\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.30965966464\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.94000566402\n",
      "\n",
      "Early stopping: Current RMSE (176822.2832) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.28317129676\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.63702183476\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.8995529231\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.74128202174\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.6369081584\n",
      "\n",
      "Early stopping: Current RMSE (187958.9787) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.97869123452\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.7408412865\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.94869928292\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.1846052833\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.80238506038\n",
      "\n",
      "Early stopping: Current RMSE (176824.1691) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.16913272828\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.29826144883\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.61233802358\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.42162224435\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.3200598163\n",
      "\n",
      "Early stopping: Current RMSE (187960.6631) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.66307038325\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.0567589693\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.21997421945\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.50358765712\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.13263388304\n",
      "\n",
      "Early stopping: Current RMSE (176822.4782) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.47823868223\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.62223880194\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.91533961598\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.7378268464\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.63512032776\n",
      "\n",
      "Early stopping: Current RMSE (187959.9776) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.977631398\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.63422485316\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.81274800198\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.08000261613\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.70518555204\n",
      "\n",
      "Early stopping: Current RMSE (176823.0580) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.05804025583\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.0546756074\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.3301658648\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.16373791554\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.06007087085\n",
      "\n",
      "Early stopping: Current RMSE (187959.4022) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.40216256463\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.8692167432\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.02746046765\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.31638674685\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.94668765887\n",
      "\n",
      "Early stopping: Current RMSE (176822.2899) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.28993790413\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.51641368738\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.83725910628\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.6422827404\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.54108951244\n",
      "\n",
      "Early stopping: Current RMSE (187960.8843) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.8842612616\n",
      "\n",
      "Training with alpha=0.8, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193205.0438253525\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173477.31273152112\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175791.48339917275\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164832.0857729972\n",
      "\n",
      "Early stopping: Current RMSE (176826.4814) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176826.48143226092\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197616.95383006323\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.19516263212\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.05023497986\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182548.94470492163\n",
      "\n",
      "Early stopping: Current RMSE (187958.2860) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.2859831492\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.22575583053\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.4464681593\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.66863746662\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.2831732861\n",
      "\n",
      "Early stopping: Current RMSE (176824.6560) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.65600868565\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197622.75959368696\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182567.1810671798\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189111.92275152734\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182554.82704719345\n",
      "\n",
      "Early stopping: Current RMSE (187964.1726) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187964.1726148969\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.82746450484\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.03761874983\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.27107087418\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.88827115743\n",
      "\n",
      "Early stopping: Current RMSE (176824.2561) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.25610632155\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.66655918706\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.9920633199\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.79415456555\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.69321544256\n",
      "\n",
      "Early stopping: Current RMSE (187961.0365) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.0364981288\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193203.2780859344\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.50018545098\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.72087235263\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.33505809997\n",
      "\n",
      "Early stopping: Current RMSE (176824.7086) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.70855045947\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.1904820282\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.53224240095\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.32410131136\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.22404888092\n",
      "\n",
      "Early stopping: Current RMSE (187961.5677) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.56771865534\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.61927685354\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.74438599308\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.06872184988\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.7073852278\n",
      "\n",
      "Early stopping: Current RMSE (176821.0349) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.03494248108\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.15459802936\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.4952449984\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.28780472995\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.1876915688\n",
      "\n",
      "Early stopping: Current RMSE (187961.5313) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.5313348316\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.512926591\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.71474290424\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.95710531386\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.57640979526\n",
      "\n",
      "Early stopping: Current RMSE (176823.9403) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.9402961511\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.2699592336\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.5521293979\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.38149683207\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.2781941241\n",
      "\n",
      "Early stopping: Current RMSE (187959.6204) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.62044489692\n",
      "\n",
      "Training with alpha=0.9, lambda=0, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.98604971234\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.14739054287\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.43300708247\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.06252636007\n",
      "\n",
      "Early stopping: Current RMSE (176822.4072) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.40724342442\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.28818282377\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.57091843756\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.3999299555\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.29665808912\n",
      "\n",
      "Early stopping: Current RMSE (187959.6389) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.6389223265\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.39266302492\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.5912913268\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.837060594\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.45716962087\n",
      "\n",
      "Early stopping: Current RMSE (176823.8195) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.81954614166\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.83193800965\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.1315454236\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.94993712855\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.84758549946\n",
      "\n",
      "Early stopping: Current RMSE (187960.1903) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.1902515153\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.87179568422\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.0566165191\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.31714110653\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.94073469276\n",
      "\n",
      "Early stopping: Current RMSE (176823.2966) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.29657200063\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.0419664117\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.28603374076\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.13938471462\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.03400381302\n",
      "\n",
      "Early stopping: Current RMSE (187958.3753) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.37534717005\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.9880892907\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.17599291084\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.43322308204\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.0560386689\n",
      "\n",
      "Early stopping: Current RMSE (176823.4133) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.41333598812\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.0649437899\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.3407526521\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.17412415796\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.07047448977\n",
      "\n",
      "Early stopping: Current RMSE (187959.4126) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.41257377242\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.79163340904\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.97432925086\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.2371247129\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.86125458276\n",
      "\n",
      "Early stopping: Current RMSE (176823.2161) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.21608548888\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197621.149463247\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182565.52097862514\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189110.29410855027\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182553.1956791291\n",
      "\n",
      "Early stopping: Current RMSE (187962.5401) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187962.54005738787\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.26814944483\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.41045954806\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.71641340773\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.35073559734\n",
      "\n",
      "Early stopping: Current RMSE (176821.6864) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.6864394995\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.38874072165\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.73665263207\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.52463951887\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.424922626\n",
      "\n",
      "Early stopping: Current RMSE (187961.7687) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.76873887464\n",
      "\n",
      "Training with alpha=0.9, lambda=0.1, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.09679164772\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.28757681802\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.54172762376\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.16381599905\n",
      "\n",
      "Early stopping: Current RMSE (176823.5225) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.52247802215\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.4726861272\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.79217476363\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.59805241047\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.49678517756\n",
      "\n",
      "Early stopping: Current RMSE (187960.8399) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.8399246197\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.06510695777\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.20203461678\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.51374047916\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.14942108726\n",
      "\n",
      "Early stopping: Current RMSE (176821.4826) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.48257578522\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.3382150272\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182561.59147455942\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.4390395686\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.33416002028\n",
      "\n",
      "Early stopping: Current RMSE (187958.6757) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187958.6757222939\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.95781335226\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.17142294702\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.4011825365\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164830.01751081907\n",
      "\n",
      "Early stopping: Current RMSE (176824.3870) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.3869824137\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.8282828953\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.09674853252\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.9347421743\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.83069199164\n",
      "\n",
      "Early stopping: Current RMSE (187959.1726) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.17261639843\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.49751527695\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.64590562184\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.9453617825\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.57814945374\n",
      "\n",
      "Early stopping: Current RMSE (176821.9167) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.91673303378\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.23206815423\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.57511885543\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.36616558197\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.2661835337\n",
      "\n",
      "Early stopping: Current RMSE (187961.6099) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.60988403135\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193201.64544630467\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173473.82426691937\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.09120364516\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164828.7163115084\n",
      "\n",
      "Early stopping: Current RMSE (176823.0693) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.0693070944\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197618.91268755437\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.21480048308\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.03161511428\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182550.92940014484\n",
      "\n",
      "Early stopping: Current RMSE (187960.2721) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.27212582415\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.2275391742\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.36877274056\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.67587705163\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.31047093673\n",
      "\n",
      "Early stopping: Current RMSE (176821.6457) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.64566497577\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.76632978406\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.0328731085\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189106.8720767351\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.76792170585\n",
      "\n",
      "Early stopping: Current RMSE (187959.1098) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.1098003334\n",
      "\n",
      "Training with alpha=0.9, lambda=0.2, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.7515394598\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.90666369983\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.1989236365\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.83001182994\n",
      "\n",
      "Early stopping: Current RMSE (176822.1718) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.17178465653\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197616.53771155613\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182560.7661324697\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189105.6293319731\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182548.52309770806\n",
      "\n",
      "Early stopping: Current RMSE (187957.8641) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187957.86406842677\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0009118819655545166, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.11929265203\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.31067429288\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.56418768616\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.18612553412\n",
      "\n",
      "Early stopping: Current RMSE (176823.5451) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.5450700413\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197617.98860544918\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182562.26204565205\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189107.09690809334\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182549.99312923418\n",
      "\n",
      "Early stopping: Current RMSE (187959.3352) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187959.33517210718\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0009118819655545166, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193199.5718275119\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173471.69567881408\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.02135887867\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164826.6603397162\n",
      "\n",
      "Early stopping: Current RMSE (176820.9873) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176820.98730123023\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197616.0484323405\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182560.26167144685\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189105.13442703983\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182548.02736476046\n",
      "\n",
      "Early stopping: Current RMSE (187957.3680) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187957.3679738969\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0024787521766663594, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.69301217474\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.84658492412\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175787.140502874\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.77198262783\n",
      "\n",
      "Early stopping: Current RMSE (176822.1130) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176822.11302065017\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197620.26870033782\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.61288765815\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189109.40321895326\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182552.30329890244\n",
      "\n",
      "Early stopping: Current RMSE (187961.6470) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.64702646292\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.0024787521766663594, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.86807700747\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173475.0793078296\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175789.31160948158\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.9285380796\n",
      "\n",
      "Early stopping: Current RMSE (176824.2969) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176824.29688309957\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.77575815984\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182564.10465051906\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.90460908072\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.80385476816\n",
      "\n",
      "Early stopping: Current RMSE (187961.1472) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187961.14721813196\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.006737946999085469, layer=5, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193202.04207428126\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173474.23140896662\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175788.48710983148\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164829.10956426477\n",
      "\n",
      "Early stopping: Current RMSE (176823.4675) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176823.467539336\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=10\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 197619.26563475985\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 182563.57869899095\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 189108.38862042275\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 182551.28700277754\n",
      "\n",
      "Early stopping: Current RMSE (187960.6300) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 187960.6299892378\n",
      "\n",
      "Training with alpha=0.9, lambda=0.3, epsilon=0.006737946999085469, layer=10, npl=20\n",
      "\n",
      "Weights are valid.\n",
      "\n",
      "Starting cross-validation...\n",
      "\n",
      "Processing fold 1...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 1: 193200.437074476\n",
      "\n",
      "Processing fold 2...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 2: 173472.58386259887\n",
      "\n",
      "Processing fold 3...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 3: 175786.88503098607\n",
      "\n",
      "Processing fold 4...\n",
      "Training stopped after reaching max_epochs (250).\n",
      "RMSE for fold 4: 164827.51822302246\n",
      "\n",
      "Early stopping: Current RMSE (176821.8560) is greater than the threshold (150000.0000)\n",
      "Skipping remaining folds for this parameter set.\n",
      "Mean RMSE: 176821.85604777085\n",
      "\n",
      "Best parameters found:\n",
      "  alpha: 0.7\n",
      "  lambda: 0.3\n",
      "  epsilon: 0.0009118819655545166\n",
      "  layer: 10\n",
      "  npl: 20\n",
      "  mean_rmse: 176820.07651027397\n",
      "  folds_completed: 4\n",
      "Parameters successfully saved to best_params.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.7,\n",
       " 'lambda': 0.3,\n",
       " 'epsilon': 0.0009118819655545166,\n",
       " 'layer': 10,\n",
       " 'npl': 20,\n",
       " 'mean_rmse': np.float64(176820.07651027397),\n",
       " 'folds_completed': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.grid_search(train_features, train_labels, params, k_fold_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: alpha=0.7, lambda=0.3, epsilon=0.0009118819655545166, layers=10, neurons=20\n",
      "Epoch 1 of 5000\n",
      "Epoch 2 of 5000\n",
      "Epoch 3 of 5000\n",
      "Epoch 4 of 5000\n",
      "Epoch 5 of 5000\n",
      "Epoch 6 of 5000\n",
      "Epoch 7 of 5000\n",
      "Epoch 8 of 5000\n",
      "Epoch 9 of 5000\n",
      "Epoch 10 of 5000\n",
      "Epoch 11 of 5000\n",
      "Epoch 12 of 5000\n",
      "Epoch 13 of 5000\n",
      "Epoch 14 of 5000\n",
      "Epoch 15 of 5000\n",
      "Epoch 16 of 5000\n",
      "Epoch 17 of 5000\n",
      "Epoch 18 of 5000\n",
      "Epoch 19 of 5000\n",
      "Epoch 20 of 5000\n",
      "Epoch 21 of 5000\n",
      "Epoch 22 of 5000\n",
      "Epoch 23 of 5000\n",
      "Epoch 24 of 5000\n",
      "Epoch 25 of 5000\n",
      "Epoch 26 of 5000\n",
      "Epoch 27 of 5000\n",
      "Epoch 28 of 5000\n",
      "Epoch 29 of 5000\n",
      "Epoch 30 of 5000\n",
      "Epoch 31 of 5000\n",
      "Epoch 32 of 5000\n",
      "Epoch 33 of 5000\n",
      "Epoch 34 of 5000\n",
      "Epoch 35 of 5000\n",
      "Epoch 36 of 5000\n",
      "Epoch 37 of 5000\n",
      "Epoch 38 of 5000\n",
      "Epoch 39 of 5000\n",
      "Epoch 40 of 5000\n",
      "Epoch 41 of 5000\n",
      "Epoch 42 of 5000\n",
      "Epoch 43 of 5000\n",
      "Epoch 44 of 5000\n",
      "Epoch 45 of 5000\n",
      "Epoch 46 of 5000\n",
      "Epoch 47 of 5000\n",
      "Epoch 48 of 5000\n",
      "Epoch 49 of 5000\n",
      "Epoch 50 of 5000\n",
      "Epoch 51 of 5000\n",
      "Epoch 52 of 5000\n",
      "Epoch 53 of 5000\n",
      "Epoch 54 of 5000\n",
      "Epoch 55 of 5000\n",
      "Epoch 56 of 5000\n",
      "Epoch 57 of 5000\n",
      "Epoch 58 of 5000\n",
      "Epoch 59 of 5000\n",
      "Epoch 60 of 5000\n",
      "Epoch 61 of 5000\n",
      "Epoch 62 of 5000\n",
      "Epoch 63 of 5000\n",
      "Epoch 64 of 5000\n",
      "Epoch 65 of 5000\n",
      "Epoch 66 of 5000\n",
      "Epoch 67 of 5000\n",
      "Epoch 68 of 5000\n",
      "Epoch 69 of 5000\n",
      "Epoch 70 of 5000\n",
      "Epoch 71 of 5000\n",
      "Epoch 72 of 5000\n",
      "Epoch 73 of 5000\n",
      "Epoch 74 of 5000\n",
      "Epoch 75 of 5000\n",
      "Epoch 76 of 5000\n",
      "Epoch 77 of 5000\n",
      "Epoch 78 of 5000\n",
      "Epoch 79 of 5000\n",
      "Epoch 80 of 5000\n",
      "Epoch 81 of 5000\n",
      "Epoch 82 of 5000\n",
      "Epoch 83 of 5000\n",
      "Epoch 84 of 5000\n",
      "Epoch 85 of 5000\n",
      "Epoch 86 of 5000\n",
      "Epoch 87 of 5000\n",
      "Epoch 88 of 5000\n",
      "Epoch 89 of 5000\n",
      "Epoch 90 of 5000\n",
      "Epoch 91 of 5000\n",
      "Epoch 92 of 5000\n",
      "Epoch 93 of 5000\n",
      "Epoch 94 of 5000\n",
      "Epoch 95 of 5000\n",
      "Epoch 96 of 5000\n",
      "Epoch 97 of 5000\n",
      "Epoch 98 of 5000\n",
      "Epoch 99 of 5000\n",
      "Epoch 100 of 5000\n",
      "Epoch 101 of 5000\n",
      "Epoch 102 of 5000\n",
      "Epoch 103 of 5000\n",
      "Epoch 104 of 5000\n",
      "Epoch 105 of 5000\n",
      "Epoch 106 of 5000\n",
      "Epoch 107 of 5000\n",
      "Epoch 108 of 5000\n",
      "Epoch 109 of 5000\n",
      "Epoch 110 of 5000\n",
      "Epoch 111 of 5000\n",
      "Epoch 112 of 5000\n",
      "Epoch 113 of 5000\n",
      "Epoch 114 of 5000\n",
      "Epoch 115 of 5000\n",
      "Epoch 116 of 5000\n",
      "Epoch 117 of 5000\n",
      "Epoch 118 of 5000\n",
      "Epoch 119 of 5000\n",
      "Epoch 120 of 5000\n",
      "Epoch 121 of 5000\n",
      "Epoch 122 of 5000\n",
      "Epoch 123 of 5000\n",
      "Epoch 124 of 5000\n",
      "Epoch 125 of 5000\n",
      "Epoch 126 of 5000\n",
      "Epoch 127 of 5000\n",
      "Epoch 128 of 5000\n",
      "Epoch 129 of 5000\n",
      "Epoch 130 of 5000\n",
      "Epoch 131 of 5000\n",
      "Epoch 132 of 5000\n",
      "Epoch 133 of 5000\n",
      "Epoch 134 of 5000\n",
      "Epoch 135 of 5000\n",
      "Epoch 136 of 5000\n",
      "Epoch 137 of 5000\n",
      "Epoch 138 of 5000\n",
      "Epoch 139 of 5000\n",
      "Epoch 140 of 5000\n",
      "Epoch 141 of 5000\n",
      "Epoch 142 of 5000\n",
      "Epoch 143 of 5000\n",
      "Epoch 144 of 5000\n",
      "Epoch 145 of 5000\n",
      "Epoch 146 of 5000\n",
      "Epoch 147 of 5000\n",
      "Epoch 148 of 5000\n",
      "Epoch 149 of 5000\n",
      "Epoch 150 of 5000\n",
      "Epoch 151 of 5000\n",
      "Epoch 152 of 5000\n",
      "Epoch 153 of 5000\n",
      "Epoch 154 of 5000\n",
      "Epoch 155 of 5000\n",
      "Epoch 156 of 5000\n",
      "Epoch 157 of 5000\n",
      "Epoch 158 of 5000\n",
      "Epoch 159 of 5000\n",
      "Epoch 160 of 5000\n",
      "Epoch 161 of 5000\n",
      "Epoch 162 of 5000\n",
      "Epoch 163 of 5000\n",
      "Epoch 164 of 5000\n",
      "Epoch 165 of 5000\n",
      "Epoch 166 of 5000\n",
      "Epoch 167 of 5000\n",
      "Epoch 168 of 5000\n",
      "Epoch 169 of 5000\n",
      "Epoch 170 of 5000\n",
      "Epoch 171 of 5000\n",
      "Epoch 172 of 5000\n",
      "Epoch 173 of 5000\n",
      "Epoch 174 of 5000\n",
      "Epoch 175 of 5000\n",
      "Epoch 176 of 5000\n",
      "Epoch 177 of 5000\n",
      "Epoch 178 of 5000\n",
      "Epoch 179 of 5000\n",
      "Epoch 180 of 5000\n",
      "Epoch 181 of 5000\n",
      "Epoch 182 of 5000\n",
      "Epoch 183 of 5000\n",
      "Epoch 184 of 5000\n",
      "Epoch 185 of 5000\n",
      "Epoch 186 of 5000\n",
      "Epoch 187 of 5000\n",
      "Epoch 188 of 5000\n",
      "Epoch 189 of 5000\n",
      "Epoch 190 of 5000\n",
      "Epoch 191 of 5000\n",
      "Epoch 192 of 5000\n",
      "Epoch 193 of 5000\n",
      "Epoch 194 of 5000\n",
      "Epoch 195 of 5000\n",
      "Epoch 196 of 5000\n",
      "Epoch 197 of 5000\n",
      "Epoch 198 of 5000\n",
      "Epoch 199 of 5000\n",
      "Epoch 200 of 5000\n",
      "Epoch 201 of 5000\n",
      "Epoch 202 of 5000\n",
      "Epoch 203 of 5000\n",
      "Epoch 204 of 5000\n",
      "Epoch 205 of 5000\n",
      "Epoch 206 of 5000\n",
      "Epoch 207 of 5000\n",
      "Epoch 208 of 5000\n",
      "Epoch 209 of 5000\n",
      "Epoch 210 of 5000\n",
      "Epoch 211 of 5000\n",
      "Epoch 212 of 5000\n",
      "Epoch 213 of 5000\n",
      "Epoch 214 of 5000\n",
      "Epoch 215 of 5000\n",
      "Epoch 216 of 5000\n",
      "Epoch 217 of 5000\n",
      "Epoch 218 of 5000\n",
      "Epoch 219 of 5000\n",
      "Epoch 220 of 5000\n",
      "Epoch 221 of 5000\n",
      "Epoch 222 of 5000\n",
      "Epoch 223 of 5000\n",
      "Epoch 224 of 5000\n",
      "Epoch 225 of 5000\n",
      "Epoch 226 of 5000\n",
      "Epoch 227 of 5000\n",
      "Epoch 228 of 5000\n",
      "Epoch 229 of 5000\n",
      "Epoch 230 of 5000\n",
      "Epoch 231 of 5000\n",
      "Epoch 232 of 5000\n",
      "Epoch 233 of 5000\n",
      "Epoch 234 of 5000\n",
      "Epoch 235 of 5000\n",
      "Epoch 236 of 5000\n",
      "Epoch 237 of 5000\n",
      "Epoch 238 of 5000\n",
      "Epoch 239 of 5000\n",
      "Epoch 240 of 5000\n",
      "Epoch 241 of 5000\n",
      "Epoch 242 of 5000\n",
      "Epoch 243 of 5000\n",
      "Epoch 244 of 5000\n",
      "Epoch 245 of 5000\n",
      "Epoch 246 of 5000\n",
      "Epoch 247 of 5000\n",
      "Epoch 248 of 5000\n",
      "Epoch 249 of 5000\n",
      "Epoch 250 of 5000\n",
      "Epoch 251 of 5000\n",
      "Epoch 252 of 5000\n",
      "Epoch 253 of 5000\n",
      "Epoch 254 of 5000\n",
      "Epoch 255 of 5000\n",
      "Epoch 256 of 5000\n",
      "Epoch 257 of 5000\n",
      "Epoch 258 of 5000\n",
      "Epoch 259 of 5000\n",
      "Epoch 260 of 5000\n",
      "Epoch 261 of 5000\n",
      "Epoch 262 of 5000\n",
      "Epoch 263 of 5000\n",
      "Epoch 264 of 5000\n",
      "Epoch 265 of 5000\n",
      "Epoch 266 of 5000\n",
      "Epoch 267 of 5000\n",
      "Epoch 268 of 5000\n",
      "Epoch 269 of 5000\n",
      "Epoch 270 of 5000\n",
      "Epoch 271 of 5000\n",
      "Epoch 272 of 5000\n",
      "Epoch 273 of 5000\n",
      "Epoch 274 of 5000\n",
      "Epoch 275 of 5000\n",
      "Epoch 276 of 5000\n",
      "Epoch 277 of 5000\n",
      "Epoch 278 of 5000\n",
      "Epoch 279 of 5000\n",
      "Epoch 280 of 5000\n",
      "Epoch 281 of 5000\n",
      "Epoch 282 of 5000\n",
      "Epoch 283 of 5000\n",
      "Epoch 284 of 5000\n",
      "Epoch 285 of 5000\n",
      "Epoch 286 of 5000\n",
      "Epoch 287 of 5000\n",
      "Epoch 288 of 5000\n",
      "Epoch 289 of 5000\n",
      "Epoch 290 of 5000\n",
      "Epoch 291 of 5000\n",
      "Epoch 292 of 5000\n",
      "Epoch 293 of 5000\n",
      "Epoch 294 of 5000\n",
      "Epoch 295 of 5000\n",
      "Epoch 296 of 5000\n",
      "Epoch 297 of 5000\n",
      "Epoch 298 of 5000\n",
      "Epoch 299 of 5000\n",
      "Epoch 300 of 5000\n",
      "Epoch 301 of 5000\n",
      "Epoch 302 of 5000\n",
      "Epoch 303 of 5000\n",
      "Epoch 304 of 5000\n",
      "Epoch 305 of 5000\n",
      "Epoch 306 of 5000\n",
      "Epoch 307 of 5000\n",
      "Epoch 308 of 5000\n",
      "Epoch 309 of 5000\n",
      "Epoch 310 of 5000\n",
      "Epoch 311 of 5000\n",
      "Epoch 312 of 5000\n",
      "Epoch 313 of 5000\n",
      "Epoch 314 of 5000\n",
      "Epoch 315 of 5000\n",
      "Epoch 316 of 5000\n",
      "Epoch 317 of 5000\n",
      "Epoch 318 of 5000\n",
      "Epoch 319 of 5000\n",
      "Epoch 320 of 5000\n",
      "Epoch 321 of 5000\n",
      "Epoch 322 of 5000\n",
      "Epoch 323 of 5000\n",
      "Epoch 324 of 5000\n",
      "Epoch 325 of 5000\n",
      "Epoch 326 of 5000\n",
      "Epoch 327 of 5000\n",
      "Epoch 328 of 5000\n",
      "Epoch 329 of 5000\n",
      "Epoch 330 of 5000\n",
      "Epoch 331 of 5000\n",
      "Epoch 332 of 5000\n",
      "Epoch 333 of 5000\n",
      "Epoch 334 of 5000\n",
      "Epoch 335 of 5000\n",
      "Epoch 336 of 5000\n",
      "Epoch 337 of 5000\n",
      "Epoch 338 of 5000\n",
      "Epoch 339 of 5000\n",
      "Epoch 340 of 5000\n",
      "Epoch 341 of 5000\n",
      "Epoch 342 of 5000\n",
      "Epoch 343 of 5000\n",
      "Epoch 344 of 5000\n",
      "Epoch 345 of 5000\n",
      "Epoch 346 of 5000\n",
      "Epoch 347 of 5000\n",
      "Epoch 348 of 5000\n",
      "Epoch 349 of 5000\n",
      "Epoch 350 of 5000\n",
      "Epoch 351 of 5000\n",
      "Epoch 352 of 5000\n",
      "Epoch 353 of 5000\n",
      "Epoch 354 of 5000\n",
      "Epoch 355 of 5000\n",
      "Epoch 356 of 5000\n",
      "Epoch 357 of 5000\n",
      "Epoch 358 of 5000\n",
      "Epoch 359 of 5000\n",
      "Epoch 360 of 5000\n",
      "Epoch 361 of 5000\n",
      "Epoch 362 of 5000\n",
      "Epoch 363 of 5000\n",
      "Epoch 364 of 5000\n",
      "Epoch 365 of 5000\n",
      "Epoch 366 of 5000\n",
      "Epoch 367 of 5000\n",
      "Epoch 368 of 5000\n",
      "Epoch 369 of 5000\n",
      "Epoch 370 of 5000\n",
      "Epoch 371 of 5000\n",
      "Epoch 372 of 5000\n",
      "Epoch 373 of 5000\n",
      "Epoch 374 of 5000\n",
      "Epoch 375 of 5000\n",
      "Epoch 376 of 5000\n",
      "Epoch 377 of 5000\n",
      "Epoch 378 of 5000\n",
      "Epoch 379 of 5000\n",
      "Epoch 380 of 5000\n",
      "Epoch 381 of 5000\n",
      "Epoch 382 of 5000\n",
      "Epoch 383 of 5000\n",
      "Epoch 384 of 5000\n",
      "Epoch 385 of 5000\n",
      "Epoch 386 of 5000\n",
      "Epoch 387 of 5000\n",
      "Epoch 388 of 5000\n",
      "Epoch 389 of 5000\n",
      "Epoch 390 of 5000\n",
      "Epoch 391 of 5000\n",
      "Epoch 392 of 5000\n",
      "Epoch 393 of 5000\n",
      "Epoch 394 of 5000\n",
      "Epoch 395 of 5000\n",
      "Epoch 396 of 5000\n",
      "Epoch 397 of 5000\n",
      "Epoch 398 of 5000\n",
      "Epoch 399 of 5000\n",
      "Epoch 400 of 5000\n",
      "Epoch 401 of 5000\n",
      "Epoch 402 of 5000\n",
      "Epoch 403 of 5000\n",
      "Epoch 404 of 5000\n",
      "Epoch 405 of 5000\n",
      "Epoch 406 of 5000\n",
      "Epoch 407 of 5000\n",
      "Epoch 408 of 5000\n",
      "Epoch 409 of 5000\n",
      "Epoch 410 of 5000\n",
      "Epoch 411 of 5000\n",
      "Epoch 412 of 5000\n",
      "Epoch 413 of 5000\n",
      "Epoch 414 of 5000\n",
      "Epoch 415 of 5000\n",
      "Epoch 416 of 5000\n",
      "Epoch 417 of 5000\n",
      "Epoch 418 of 5000\n",
      "Epoch 419 of 5000\n",
      "Epoch 420 of 5000\n",
      "Epoch 421 of 5000\n",
      "Epoch 422 of 5000\n",
      "Epoch 423 of 5000\n",
      "Epoch 424 of 5000\n",
      "Epoch 425 of 5000\n",
      "Epoch 426 of 5000\n",
      "Epoch 427 of 5000\n",
      "Epoch 428 of 5000\n",
      "Epoch 429 of 5000\n",
      "Epoch 430 of 5000\n",
      "Epoch 431 of 5000\n",
      "Epoch 432 of 5000\n",
      "Epoch 433 of 5000\n",
      "Epoch 434 of 5000\n",
      "Epoch 435 of 5000\n",
      "Epoch 436 of 5000\n",
      "Epoch 437 of 5000\n",
      "Epoch 438 of 5000\n",
      "Epoch 439 of 5000\n",
      "Epoch 440 of 5000\n",
      "Epoch 441 of 5000\n",
      "Epoch 442 of 5000\n",
      "Epoch 443 of 5000\n",
      "Epoch 444 of 5000\n",
      "Epoch 445 of 5000\n",
      "Epoch 446 of 5000\n",
      "Epoch 447 of 5000\n",
      "Epoch 448 of 5000\n",
      "Epoch 449 of 5000\n",
      "Epoch 450 of 5000\n",
      "Epoch 451 of 5000\n",
      "Epoch 452 of 5000\n",
      "Epoch 453 of 5000\n",
      "Epoch 454 of 5000\n",
      "Epoch 455 of 5000\n",
      "Epoch 456 of 5000\n",
      "Epoch 457 of 5000\n",
      "Epoch 458 of 5000\n",
      "Epoch 459 of 5000\n",
      "Epoch 460 of 5000\n",
      "Epoch 461 of 5000\n",
      "Epoch 462 of 5000\n",
      "Epoch 463 of 5000\n",
      "Epoch 464 of 5000\n",
      "Epoch 465 of 5000\n",
      "Epoch 466 of 5000\n",
      "Epoch 467 of 5000\n",
      "Epoch 468 of 5000\n",
      "Epoch 469 of 5000\n",
      "Epoch 470 of 5000\n",
      "Epoch 471 of 5000\n",
      "Epoch 472 of 5000\n",
      "Epoch 473 of 5000\n",
      "Epoch 474 of 5000\n",
      "Epoch 475 of 5000\n",
      "Epoch 476 of 5000\n",
      "Epoch 477 of 5000\n",
      "Epoch 478 of 5000\n",
      "Epoch 479 of 5000\n",
      "Epoch 480 of 5000\n",
      "Epoch 481 of 5000\n",
      "Epoch 482 of 5000\n",
      "Epoch 483 of 5000\n",
      "Epoch 484 of 5000\n",
      "Epoch 485 of 5000\n",
      "Epoch 486 of 5000\n",
      "Epoch 487 of 5000\n",
      "Epoch 488 of 5000\n",
      "Epoch 489 of 5000\n",
      "Epoch 490 of 5000\n",
      "Epoch 491 of 5000\n",
      "Epoch 492 of 5000\n",
      "Epoch 493 of 5000\n",
      "Epoch 494 of 5000\n",
      "Epoch 495 of 5000\n",
      "Epoch 496 of 5000\n",
      "Epoch 497 of 5000\n",
      "Epoch 498 of 5000\n",
      "Epoch 499 of 5000\n",
      "Epoch 500 of 5000\n",
      "Epoch 501 of 5000\n",
      "Epoch 502 of 5000\n",
      "Epoch 503 of 5000\n",
      "Epoch 504 of 5000\n",
      "Epoch 505 of 5000\n",
      "Epoch 506 of 5000\n",
      "Epoch 507 of 5000\n",
      "Epoch 508 of 5000\n",
      "Epoch 509 of 5000\n",
      "Epoch 510 of 5000\n",
      "Epoch 511 of 5000\n",
      "Epoch 512 of 5000\n",
      "Epoch 513 of 5000\n",
      "Epoch 514 of 5000\n",
      "Epoch 515 of 5000\n",
      "Epoch 516 of 5000\n",
      "Epoch 517 of 5000\n",
      "Epoch 518 of 5000\n",
      "Epoch 519 of 5000\n",
      "Epoch 520 of 5000\n",
      "Epoch 521 of 5000\n",
      "Epoch 522 of 5000\n",
      "Epoch 523 of 5000\n",
      "Epoch 524 of 5000\n",
      "Epoch 525 of 5000\n",
      "Epoch 526 of 5000\n",
      "Epoch 527 of 5000\n",
      "Epoch 528 of 5000\n",
      "Epoch 529 of 5000\n",
      "Epoch 530 of 5000\n",
      "Epoch 531 of 5000\n",
      "Epoch 532 of 5000\n",
      "Epoch 533 of 5000\n",
      "Epoch 534 of 5000\n",
      "Epoch 535 of 5000\n",
      "Epoch 536 of 5000\n",
      "Epoch 537 of 5000\n",
      "Epoch 538 of 5000\n",
      "Epoch 539 of 5000\n",
      "Epoch 540 of 5000\n",
      "Epoch 541 of 5000\n",
      "Epoch 542 of 5000\n",
      "Epoch 543 of 5000\n",
      "Epoch 544 of 5000\n",
      "Epoch 545 of 5000\n",
      "Epoch 546 of 5000\n",
      "Epoch 547 of 5000\n",
      "Epoch 548 of 5000\n",
      "Epoch 549 of 5000\n",
      "Epoch 550 of 5000\n",
      "Epoch 551 of 5000\n",
      "Epoch 552 of 5000\n",
      "Epoch 553 of 5000\n",
      "Epoch 554 of 5000\n",
      "Epoch 555 of 5000\n",
      "Epoch 556 of 5000\n",
      "Epoch 557 of 5000\n",
      "Epoch 558 of 5000\n",
      "Epoch 559 of 5000\n",
      "Epoch 560 of 5000\n",
      "Epoch 561 of 5000\n",
      "Epoch 562 of 5000\n",
      "Epoch 563 of 5000\n",
      "Epoch 564 of 5000\n",
      "Epoch 565 of 5000\n",
      "Epoch 566 of 5000\n",
      "Epoch 567 of 5000\n",
      "Epoch 568 of 5000\n",
      "Epoch 569 of 5000\n",
      "Epoch 570 of 5000\n",
      "Epoch 571 of 5000\n",
      "Epoch 572 of 5000\n",
      "Epoch 573 of 5000\n",
      "Epoch 574 of 5000\n",
      "Epoch 575 of 5000\n",
      "Epoch 576 of 5000\n",
      "Epoch 577 of 5000\n",
      "Epoch 578 of 5000\n",
      "Epoch 579 of 5000\n",
      "Epoch 580 of 5000\n",
      "Epoch 581 of 5000\n",
      "Epoch 582 of 5000\n",
      "Epoch 583 of 5000\n",
      "Epoch 584 of 5000\n",
      "Epoch 585 of 5000\n",
      "Epoch 586 of 5000\n",
      "Epoch 587 of 5000\n",
      "Epoch 588 of 5000\n",
      "Epoch 589 of 5000\n",
      "Epoch 590 of 5000\n",
      "Epoch 591 of 5000\n",
      "Epoch 592 of 5000\n",
      "Epoch 593 of 5000\n",
      "Epoch 594 of 5000\n",
      "Epoch 595 of 5000\n",
      "Epoch 596 of 5000\n",
      "Epoch 597 of 5000\n",
      "Epoch 598 of 5000\n",
      "Epoch 599 of 5000\n",
      "Epoch 600 of 5000\n",
      "Epoch 601 of 5000\n",
      "Epoch 602 of 5000\n",
      "Epoch 603 of 5000\n",
      "Epoch 604 of 5000\n",
      "Epoch 605 of 5000\n",
      "Epoch 606 of 5000\n",
      "Epoch 607 of 5000\n",
      "Epoch 608 of 5000\n",
      "Epoch 609 of 5000\n",
      "Epoch 610 of 5000\n",
      "Epoch 611 of 5000\n",
      "Epoch 612 of 5000\n",
      "Epoch 613 of 5000\n",
      "Epoch 614 of 5000\n",
      "Epoch 615 of 5000\n",
      "Epoch 616 of 5000\n",
      "Epoch 617 of 5000\n",
      "Epoch 618 of 5000\n",
      "Epoch 619 of 5000\n",
      "Epoch 620 of 5000\n",
      "Epoch 621 of 5000\n",
      "Epoch 622 of 5000\n",
      "Epoch 623 of 5000\n",
      "Epoch 624 of 5000\n",
      "Epoch 625 of 5000\n",
      "Epoch 626 of 5000\n",
      "Epoch 627 of 5000\n",
      "Epoch 628 of 5000\n",
      "Epoch 629 of 5000\n",
      "Epoch 630 of 5000\n",
      "Epoch 631 of 5000\n",
      "Epoch 632 of 5000\n",
      "Epoch 633 of 5000\n",
      "Epoch 634 of 5000\n",
      "Epoch 635 of 5000\n",
      "Epoch 636 of 5000\n",
      "Epoch 637 of 5000\n",
      "Epoch 638 of 5000\n",
      "Epoch 639 of 5000\n",
      "Epoch 640 of 5000\n",
      "Epoch 641 of 5000\n",
      "Epoch 642 of 5000\n",
      "Epoch 643 of 5000\n",
      "Epoch 644 of 5000\n",
      "Epoch 645 of 5000\n",
      "Epoch 646 of 5000\n",
      "Epoch 647 of 5000\n",
      "Epoch 648 of 5000\n",
      "Epoch 649 of 5000\n",
      "Epoch 650 of 5000\n",
      "Epoch 651 of 5000\n",
      "Epoch 652 of 5000\n",
      "Epoch 653 of 5000\n",
      "Epoch 654 of 5000\n",
      "Epoch 655 of 5000\n",
      "Epoch 656 of 5000\n",
      "Epoch 657 of 5000\n",
      "Epoch 658 of 5000\n",
      "Epoch 659 of 5000\n",
      "Epoch 660 of 5000\n",
      "Epoch 661 of 5000\n",
      "Epoch 662 of 5000\n",
      "Epoch 663 of 5000\n",
      "Epoch 664 of 5000\n",
      "Epoch 665 of 5000\n",
      "Epoch 666 of 5000\n",
      "Epoch 667 of 5000\n",
      "Epoch 668 of 5000\n",
      "Epoch 669 of 5000\n",
      "Epoch 670 of 5000\n",
      "Epoch 671 of 5000\n",
      "Epoch 672 of 5000\n",
      "Epoch 673 of 5000\n",
      "Epoch 674 of 5000\n",
      "Epoch 675 of 5000\n",
      "Epoch 676 of 5000\n",
      "Epoch 677 of 5000\n",
      "Epoch 678 of 5000\n",
      "Epoch 679 of 5000\n",
      "Epoch 680 of 5000\n",
      "Epoch 681 of 5000\n",
      "Epoch 682 of 5000\n",
      "Epoch 683 of 5000\n",
      "Epoch 684 of 5000\n",
      "Epoch 685 of 5000\n",
      "Epoch 686 of 5000\n",
      "Epoch 687 of 5000\n",
      "Epoch 688 of 5000\n",
      "Epoch 689 of 5000\n",
      "Epoch 690 of 5000\n",
      "Epoch 691 of 5000\n",
      "Epoch 692 of 5000\n",
      "Epoch 693 of 5000\n",
      "Epoch 694 of 5000\n",
      "Epoch 695 of 5000\n",
      "Epoch 696 of 5000\n",
      "Epoch 697 of 5000\n",
      "Epoch 698 of 5000\n",
      "Epoch 699 of 5000\n",
      "Epoch 700 of 5000\n",
      "Epoch 701 of 5000\n",
      "Epoch 702 of 5000\n",
      "Epoch 703 of 5000\n",
      "Epoch 704 of 5000\n",
      "Epoch 705 of 5000\n",
      "Epoch 706 of 5000\n",
      "Epoch 707 of 5000\n",
      "Epoch 708 of 5000\n",
      "Epoch 709 of 5000\n",
      "Epoch 710 of 5000\n",
      "Epoch 711 of 5000\n",
      "Epoch 712 of 5000\n",
      "Epoch 713 of 5000\n",
      "Epoch 714 of 5000\n",
      "Epoch 715 of 5000\n",
      "Epoch 716 of 5000\n",
      "Epoch 717 of 5000\n",
      "Epoch 718 of 5000\n",
      "Epoch 719 of 5000\n",
      "Epoch 720 of 5000\n",
      "Epoch 721 of 5000\n",
      "Epoch 722 of 5000\n",
      "Epoch 723 of 5000\n",
      "Epoch 724 of 5000\n",
      "Epoch 725 of 5000\n",
      "Epoch 726 of 5000\n",
      "Epoch 727 of 5000\n",
      "Epoch 728 of 5000\n",
      "Epoch 729 of 5000\n",
      "Epoch 730 of 5000\n",
      "Epoch 731 of 5000\n",
      "Epoch 732 of 5000\n",
      "Epoch 733 of 5000\n",
      "Epoch 734 of 5000\n",
      "Epoch 735 of 5000\n",
      "Epoch 736 of 5000\n",
      "Epoch 737 of 5000\n",
      "Epoch 738 of 5000\n",
      "Epoch 739 of 5000\n",
      "Epoch 740 of 5000\n",
      "Epoch 741 of 5000\n",
      "Epoch 742 of 5000\n",
      "Epoch 743 of 5000\n",
      "Epoch 744 of 5000\n",
      "Epoch 745 of 5000\n",
      "Epoch 746 of 5000\n",
      "Epoch 747 of 5000\n",
      "Epoch 748 of 5000\n",
      "Epoch 749 of 5000\n",
      "Epoch 750 of 5000\n",
      "Epoch 751 of 5000\n",
      "Epoch 752 of 5000\n",
      "Epoch 753 of 5000\n",
      "Epoch 754 of 5000\n",
      "Epoch 755 of 5000\n",
      "Epoch 756 of 5000\n",
      "Epoch 757 of 5000\n",
      "Epoch 758 of 5000\n",
      "Epoch 759 of 5000\n",
      "Epoch 760 of 5000\n",
      "Epoch 761 of 5000\n",
      "Epoch 762 of 5000\n",
      "Epoch 763 of 5000\n",
      "Epoch 764 of 5000\n",
      "Epoch 765 of 5000\n",
      "Epoch 766 of 5000\n",
      "Epoch 767 of 5000\n",
      "Epoch 768 of 5000\n",
      "Epoch 769 of 5000\n",
      "Epoch 770 of 5000\n",
      "Epoch 771 of 5000\n",
      "Epoch 772 of 5000\n",
      "Epoch 773 of 5000\n",
      "Epoch 774 of 5000\n",
      "Epoch 775 of 5000\n",
      "Epoch 776 of 5000\n",
      "Epoch 777 of 5000\n",
      "Epoch 778 of 5000\n",
      "Epoch 779 of 5000\n",
      "Epoch 780 of 5000\n",
      "Epoch 781 of 5000\n",
      "Epoch 782 of 5000\n",
      "Epoch 783 of 5000\n",
      "Epoch 784 of 5000\n",
      "Epoch 785 of 5000\n",
      "Epoch 786 of 5000\n",
      "Epoch 787 of 5000\n",
      "Epoch 788 of 5000\n",
      "Epoch 789 of 5000\n",
      "Epoch 790 of 5000\n",
      "Epoch 791 of 5000\n",
      "Epoch 792 of 5000\n",
      "Epoch 793 of 5000\n",
      "Epoch 794 of 5000\n",
      "Epoch 795 of 5000\n",
      "Epoch 796 of 5000\n",
      "Epoch 797 of 5000\n",
      "Epoch 798 of 5000\n",
      "Epoch 799 of 5000\n",
      "Epoch 800 of 5000\n",
      "Epoch 801 of 5000\n",
      "Epoch 802 of 5000\n",
      "Epoch 803 of 5000\n",
      "Epoch 804 of 5000\n",
      "Epoch 805 of 5000\n",
      "Epoch 806 of 5000\n",
      "Epoch 807 of 5000\n",
      "Epoch 808 of 5000\n",
      "Epoch 809 of 5000\n",
      "Epoch 810 of 5000\n",
      "Epoch 811 of 5000\n",
      "Epoch 812 of 5000\n",
      "Epoch 813 of 5000\n",
      "Epoch 814 of 5000\n",
      "Epoch 815 of 5000\n",
      "Epoch 816 of 5000\n",
      "Epoch 817 of 5000\n",
      "Epoch 818 of 5000\n",
      "Epoch 819 of 5000\n",
      "Epoch 820 of 5000\n",
      "Epoch 821 of 5000\n",
      "Epoch 822 of 5000\n",
      "Epoch 823 of 5000\n",
      "Epoch 824 of 5000\n",
      "Epoch 825 of 5000\n",
      "Epoch 826 of 5000\n",
      "Epoch 827 of 5000\n",
      "Epoch 828 of 5000\n",
      "Epoch 829 of 5000\n",
      "Epoch 830 of 5000\n",
      "Epoch 831 of 5000\n",
      "Epoch 832 of 5000\n",
      "Epoch 833 of 5000\n",
      "Epoch 834 of 5000\n",
      "Epoch 835 of 5000\n",
      "Epoch 836 of 5000\n",
      "Epoch 837 of 5000\n",
      "Epoch 838 of 5000\n",
      "Epoch 839 of 5000\n",
      "Epoch 840 of 5000\n",
      "Epoch 841 of 5000\n",
      "Epoch 842 of 5000\n",
      "Epoch 843 of 5000\n",
      "Epoch 844 of 5000\n",
      "Epoch 845 of 5000\n",
      "Epoch 846 of 5000\n",
      "Epoch 847 of 5000\n",
      "Epoch 848 of 5000\n",
      "Epoch 849 of 5000\n",
      "Epoch 850 of 5000\n",
      "Epoch 851 of 5000\n",
      "Epoch 852 of 5000\n",
      "Epoch 853 of 5000\n",
      "Epoch 854 of 5000\n",
      "Epoch 855 of 5000\n",
      "Epoch 856 of 5000\n",
      "Epoch 857 of 5000\n",
      "Epoch 858 of 5000\n",
      "Epoch 859 of 5000\n",
      "Epoch 860 of 5000\n",
      "Epoch 861 of 5000\n",
      "Epoch 862 of 5000\n",
      "Epoch 863 of 5000\n",
      "Epoch 864 of 5000\n",
      "Epoch 865 of 5000\n",
      "Epoch 866 of 5000\n",
      "Epoch 867 of 5000\n",
      "Epoch 868 of 5000\n",
      "Epoch 869 of 5000\n",
      "Epoch 870 of 5000\n",
      "Epoch 871 of 5000\n",
      "Epoch 872 of 5000\n",
      "Epoch 873 of 5000\n",
      "Epoch 874 of 5000\n",
      "Epoch 875 of 5000\n",
      "Epoch 876 of 5000\n",
      "Epoch 877 of 5000\n",
      "Epoch 878 of 5000\n",
      "Epoch 879 of 5000\n",
      "Epoch 880 of 5000\n",
      "Epoch 881 of 5000\n",
      "Epoch 882 of 5000\n",
      "Epoch 883 of 5000\n",
      "Epoch 884 of 5000\n",
      "Epoch 885 of 5000\n",
      "Epoch 886 of 5000\n",
      "Epoch 887 of 5000\n",
      "Epoch 888 of 5000\n",
      "Epoch 889 of 5000\n",
      "Epoch 890 of 5000\n",
      "Epoch 891 of 5000\n",
      "Epoch 892 of 5000\n",
      "Epoch 893 of 5000\n",
      "Epoch 894 of 5000\n",
      "Epoch 895 of 5000\n",
      "Epoch 896 of 5000\n",
      "Epoch 897 of 5000\n",
      "Epoch 898 of 5000\n",
      "Epoch 899 of 5000\n",
      "Epoch 900 of 5000\n",
      "Epoch 901 of 5000\n",
      "Epoch 902 of 5000\n",
      "Epoch 903 of 5000\n",
      "Epoch 904 of 5000\n",
      "Epoch 905 of 5000\n",
      "Epoch 906 of 5000\n",
      "Epoch 907 of 5000\n",
      "Epoch 908 of 5000\n",
      "Epoch 909 of 5000\n",
      "Epoch 910 of 5000\n",
      "Epoch 911 of 5000\n",
      "Epoch 912 of 5000\n",
      "Epoch 913 of 5000\n",
      "Epoch 914 of 5000\n",
      "Epoch 915 of 5000\n",
      "Epoch 916 of 5000\n",
      "Epoch 917 of 5000\n",
      "Epoch 918 of 5000\n",
      "Epoch 919 of 5000\n",
      "Epoch 920 of 5000\n",
      "Epoch 921 of 5000\n",
      "Epoch 922 of 5000\n",
      "Epoch 923 of 5000\n",
      "Epoch 924 of 5000\n",
      "Epoch 925 of 5000\n",
      "Epoch 926 of 5000\n",
      "Epoch 927 of 5000\n",
      "Epoch 928 of 5000\n",
      "Epoch 929 of 5000\n",
      "Epoch 930 of 5000\n",
      "Epoch 931 of 5000\n",
      "Epoch 932 of 5000\n",
      "Epoch 933 of 5000\n",
      "Epoch 934 of 5000\n",
      "Epoch 935 of 5000\n",
      "Epoch 936 of 5000\n",
      "Epoch 937 of 5000\n",
      "Epoch 938 of 5000\n",
      "Epoch 939 of 5000\n",
      "Epoch 940 of 5000\n",
      "Epoch 941 of 5000\n",
      "Epoch 942 of 5000\n",
      "Epoch 943 of 5000\n",
      "Epoch 944 of 5000\n",
      "Epoch 945 of 5000\n",
      "Epoch 946 of 5000\n",
      "Epoch 947 of 5000\n",
      "Epoch 948 of 5000\n",
      "Epoch 949 of 5000\n",
      "Epoch 950 of 5000\n",
      "Epoch 951 of 5000\n",
      "Epoch 952 of 5000\n",
      "Epoch 953 of 5000\n",
      "Epoch 954 of 5000\n",
      "Epoch 955 of 5000\n",
      "Epoch 956 of 5000\n",
      "Epoch 957 of 5000\n",
      "Epoch 958 of 5000\n",
      "Epoch 959 of 5000\n",
      "Epoch 960 of 5000\n",
      "Epoch 961 of 5000\n",
      "Epoch 962 of 5000\n",
      "Epoch 963 of 5000\n",
      "Epoch 964 of 5000\n",
      "Epoch 965 of 5000\n",
      "Epoch 966 of 5000\n",
      "Epoch 967 of 5000\n",
      "Epoch 968 of 5000\n",
      "Epoch 969 of 5000\n",
      "Epoch 970 of 5000\n",
      "Epoch 971 of 5000\n",
      "Epoch 972 of 5000\n",
      "Epoch 973 of 5000\n",
      "Epoch 974 of 5000\n",
      "Epoch 975 of 5000\n",
      "Epoch 976 of 5000\n",
      "Epoch 977 of 5000\n",
      "Epoch 978 of 5000\n",
      "Epoch 979 of 5000\n",
      "Epoch 980 of 5000\n",
      "Epoch 981 of 5000\n",
      "Epoch 982 of 5000\n",
      "Epoch 983 of 5000\n",
      "Epoch 984 of 5000\n",
      "Epoch 985 of 5000\n",
      "Epoch 986 of 5000\n",
      "Epoch 987 of 5000\n",
      "Epoch 988 of 5000\n",
      "Epoch 989 of 5000\n",
      "Epoch 990 of 5000\n",
      "Epoch 991 of 5000\n",
      "Epoch 992 of 5000\n",
      "Epoch 993 of 5000\n",
      "Epoch 994 of 5000\n",
      "Epoch 995 of 5000\n",
      "Epoch 996 of 5000\n",
      "Epoch 997 of 5000\n",
      "Epoch 998 of 5000\n",
      "Epoch 999 of 5000\n",
      "Epoch 1000 of 5000\n",
      "Epoch 1001 of 5000\n",
      "Epoch 1002 of 5000\n",
      "Epoch 1003 of 5000\n",
      "Epoch 1004 of 5000\n",
      "Epoch 1005 of 5000\n",
      "Epoch 1006 of 5000\n",
      "Epoch 1007 of 5000\n",
      "Epoch 1008 of 5000\n",
      "Epoch 1009 of 5000\n",
      "Epoch 1010 of 5000\n",
      "Epoch 1011 of 5000\n",
      "Epoch 1012 of 5000\n",
      "Epoch 1013 of 5000\n",
      "Epoch 1014 of 5000\n",
      "Epoch 1015 of 5000\n",
      "Epoch 1016 of 5000\n",
      "Epoch 1017 of 5000\n",
      "Epoch 1018 of 5000\n",
      "Epoch 1019 of 5000\n",
      "Epoch 1020 of 5000\n",
      "Epoch 1021 of 5000\n",
      "Epoch 1022 of 5000\n",
      "Epoch 1023 of 5000\n",
      "Epoch 1024 of 5000\n",
      "Epoch 1025 of 5000\n",
      "Epoch 1026 of 5000\n",
      "Epoch 1027 of 5000\n",
      "Epoch 1028 of 5000\n",
      "Epoch 1029 of 5000\n",
      "Epoch 1030 of 5000\n",
      "Epoch 1031 of 5000\n",
      "Epoch 1032 of 5000\n",
      "Epoch 1033 of 5000\n",
      "Epoch 1034 of 5000\n",
      "Epoch 1035 of 5000\n",
      "Epoch 1036 of 5000\n",
      "Epoch 1037 of 5000\n",
      "Epoch 1038 of 5000\n",
      "Epoch 1039 of 5000\n",
      "Epoch 1040 of 5000\n",
      "Epoch 1041 of 5000\n",
      "Epoch 1042 of 5000\n",
      "Epoch 1043 of 5000\n",
      "Epoch 1044 of 5000\n",
      "Epoch 1045 of 5000\n",
      "Epoch 1046 of 5000\n",
      "Epoch 1047 of 5000\n",
      "Epoch 1048 of 5000\n",
      "Epoch 1049 of 5000\n",
      "Epoch 1050 of 5000\n",
      "Epoch 1051 of 5000\n",
      "Epoch 1052 of 5000\n",
      "Epoch 1053 of 5000\n",
      "Epoch 1054 of 5000\n",
      "Epoch 1055 of 5000\n",
      "Epoch 1056 of 5000\n",
      "Epoch 1057 of 5000\n",
      "Epoch 1058 of 5000\n",
      "Epoch 1059 of 5000\n",
      "Epoch 1060 of 5000\n",
      "Epoch 1061 of 5000\n",
      "Epoch 1062 of 5000\n",
      "Epoch 1063 of 5000\n",
      "Epoch 1064 of 5000\n",
      "Epoch 1065 of 5000\n",
      "Epoch 1066 of 5000\n",
      "Epoch 1067 of 5000\n",
      "Epoch 1068 of 5000\n",
      "Epoch 1069 of 5000\n",
      "Epoch 1070 of 5000\n",
      "Epoch 1071 of 5000\n",
      "Epoch 1072 of 5000\n",
      "Epoch 1073 of 5000\n",
      "Epoch 1074 of 5000\n",
      "Epoch 1075 of 5000\n",
      "Epoch 1076 of 5000\n",
      "Epoch 1077 of 5000\n",
      "Epoch 1078 of 5000\n",
      "Epoch 1079 of 5000\n",
      "Epoch 1080 of 5000\n",
      "Epoch 1081 of 5000\n",
      "Epoch 1082 of 5000\n",
      "Epoch 1083 of 5000\n",
      "Epoch 1084 of 5000\n",
      "Epoch 1085 of 5000\n",
      "Epoch 1086 of 5000\n",
      "Epoch 1087 of 5000\n",
      "Epoch 1088 of 5000\n",
      "Epoch 1089 of 5000\n",
      "Epoch 1090 of 5000\n",
      "Epoch 1091 of 5000\n",
      "Epoch 1092 of 5000\n",
      "Epoch 1093 of 5000\n",
      "Epoch 1094 of 5000\n",
      "Epoch 1095 of 5000\n",
      "Epoch 1096 of 5000\n",
      "Epoch 1097 of 5000\n",
      "Epoch 1098 of 5000\n",
      "Epoch 1099 of 5000\n",
      "Epoch 1100 of 5000\n",
      "Epoch 1101 of 5000\n",
      "Epoch 1102 of 5000\n",
      "Epoch 1103 of 5000\n",
      "Epoch 1104 of 5000\n",
      "Epoch 1105 of 5000\n",
      "Epoch 1106 of 5000\n",
      "Epoch 1107 of 5000\n",
      "Epoch 1108 of 5000\n",
      "Epoch 1109 of 5000\n",
      "Epoch 1110 of 5000\n",
      "Epoch 1111 of 5000\n",
      "Epoch 1112 of 5000\n",
      "Epoch 1113 of 5000\n",
      "Epoch 1114 of 5000\n",
      "Epoch 1115 of 5000\n",
      "Epoch 1116 of 5000\n",
      "Epoch 1117 of 5000\n",
      "Epoch 1118 of 5000\n",
      "Epoch 1119 of 5000\n",
      "Epoch 1120 of 5000\n",
      "Epoch 1121 of 5000\n",
      "Epoch 1122 of 5000\n",
      "Epoch 1123 of 5000\n",
      "Epoch 1124 of 5000\n",
      "Epoch 1125 of 5000\n",
      "Epoch 1126 of 5000\n",
      "Epoch 1127 of 5000\n",
      "Epoch 1128 of 5000\n",
      "Epoch 1129 of 5000\n",
      "Epoch 1130 of 5000\n",
      "Epoch 1131 of 5000\n",
      "Epoch 1132 of 5000\n",
      "Epoch 1133 of 5000\n",
      "Epoch 1134 of 5000\n",
      "Epoch 1135 of 5000\n",
      "Epoch 1136 of 5000\n",
      "Epoch 1137 of 5000\n",
      "Epoch 1138 of 5000\n",
      "Epoch 1139 of 5000\n",
      "Epoch 1140 of 5000\n",
      "Epoch 1141 of 5000\n",
      "Epoch 1142 of 5000\n",
      "Epoch 1143 of 5000\n",
      "Epoch 1144 of 5000\n",
      "Epoch 1145 of 5000\n",
      "Epoch 1146 of 5000\n",
      "Epoch 1147 of 5000\n",
      "Epoch 1148 of 5000\n",
      "Epoch 1149 of 5000\n",
      "Epoch 1150 of 5000\n",
      "Epoch 1151 of 5000\n",
      "Epoch 1152 of 5000\n",
      "Epoch 1153 of 5000\n",
      "Epoch 1154 of 5000\n",
      "Epoch 1155 of 5000\n",
      "Epoch 1156 of 5000\n",
      "Epoch 1157 of 5000\n",
      "Epoch 1158 of 5000\n",
      "Epoch 1159 of 5000\n",
      "Epoch 1160 of 5000\n",
      "Epoch 1161 of 5000\n",
      "Epoch 1162 of 5000\n",
      "Epoch 1163 of 5000\n",
      "Epoch 1164 of 5000\n",
      "Epoch 1165 of 5000\n",
      "Epoch 1166 of 5000\n",
      "Epoch 1167 of 5000\n",
      "Epoch 1168 of 5000\n",
      "Epoch 1169 of 5000\n",
      "Epoch 1170 of 5000\n",
      "Epoch 1171 of 5000\n",
      "Epoch 1172 of 5000\n",
      "Epoch 1173 of 5000\n",
      "Epoch 1174 of 5000\n",
      "Epoch 1175 of 5000\n",
      "Epoch 1176 of 5000\n",
      "Epoch 1177 of 5000\n",
      "Epoch 1178 of 5000\n",
      "Epoch 1179 of 5000\n",
      "Epoch 1180 of 5000\n",
      "Epoch 1181 of 5000\n",
      "Epoch 1182 of 5000\n",
      "Epoch 1183 of 5000\n",
      "Epoch 1184 of 5000\n",
      "Epoch 1185 of 5000\n",
      "Epoch 1186 of 5000\n",
      "Epoch 1187 of 5000\n",
      "Epoch 1188 of 5000\n",
      "Epoch 1189 of 5000\n",
      "Epoch 1190 of 5000\n",
      "Epoch 1191 of 5000\n",
      "Epoch 1192 of 5000\n",
      "Epoch 1193 of 5000\n",
      "Epoch 1194 of 5000\n",
      "Epoch 1195 of 5000\n",
      "Epoch 1196 of 5000\n",
      "Epoch 1197 of 5000\n",
      "Epoch 1198 of 5000\n",
      "Epoch 1199 of 5000\n",
      "Epoch 1200 of 5000\n",
      "Epoch 1201 of 5000\n",
      "Epoch 1202 of 5000\n",
      "Epoch 1203 of 5000\n",
      "Epoch 1204 of 5000\n",
      "Epoch 1205 of 5000\n",
      "Epoch 1206 of 5000\n",
      "Epoch 1207 of 5000\n",
      "Epoch 1208 of 5000\n",
      "Epoch 1209 of 5000\n",
      "Epoch 1210 of 5000\n",
      "Epoch 1211 of 5000\n",
      "Epoch 1212 of 5000\n",
      "Epoch 1213 of 5000\n",
      "Epoch 1214 of 5000\n",
      "Epoch 1215 of 5000\n",
      "Epoch 1216 of 5000\n",
      "Epoch 1217 of 5000\n",
      "Epoch 1218 of 5000\n",
      "Epoch 1219 of 5000\n",
      "Epoch 1220 of 5000\n",
      "Epoch 1221 of 5000\n",
      "Epoch 1222 of 5000\n",
      "Epoch 1223 of 5000\n",
      "Epoch 1224 of 5000\n",
      "Epoch 1225 of 5000\n",
      "Epoch 1226 of 5000\n",
      "Epoch 1227 of 5000\n",
      "Epoch 1228 of 5000\n",
      "Epoch 1229 of 5000\n",
      "Epoch 1230 of 5000\n",
      "Epoch 1231 of 5000\n",
      "Epoch 1232 of 5000\n",
      "Epoch 1233 of 5000\n",
      "Epoch 1234 of 5000\n",
      "Epoch 1235 of 5000\n",
      "Epoch 1236 of 5000\n",
      "Epoch 1237 of 5000\n",
      "Epoch 1238 of 5000\n",
      "Epoch 1239 of 5000\n",
      "Epoch 1240 of 5000\n",
      "Epoch 1241 of 5000\n",
      "Epoch 1242 of 5000\n",
      "Epoch 1243 of 5000\n",
      "Epoch 1244 of 5000\n",
      "Epoch 1245 of 5000\n",
      "Epoch 1246 of 5000\n",
      "Epoch 1247 of 5000\n",
      "Epoch 1248 of 5000\n",
      "Epoch 1249 of 5000\n",
      "Epoch 1250 of 5000\n",
      "Epoch 1251 of 5000\n",
      "Epoch 1252 of 5000\n",
      "Epoch 1253 of 5000\n",
      "Epoch 1254 of 5000\n",
      "Epoch 1255 of 5000\n",
      "Epoch 1256 of 5000\n",
      "Epoch 1257 of 5000\n",
      "Epoch 1258 of 5000\n",
      "Epoch 1259 of 5000\n",
      "Epoch 1260 of 5000\n",
      "Epoch 1261 of 5000\n",
      "Epoch 1262 of 5000\n",
      "Epoch 1263 of 5000\n",
      "Epoch 1264 of 5000\n",
      "Epoch 1265 of 5000\n",
      "Epoch 1266 of 5000\n",
      "Epoch 1267 of 5000\n",
      "Epoch 1268 of 5000\n",
      "Epoch 1269 of 5000\n",
      "Epoch 1270 of 5000\n",
      "Epoch 1271 of 5000\n",
      "Epoch 1272 of 5000\n",
      "Epoch 1273 of 5000\n",
      "Epoch 1274 of 5000\n",
      "Epoch 1275 of 5000\n",
      "Epoch 1276 of 5000\n",
      "Epoch 1277 of 5000\n",
      "Epoch 1278 of 5000\n",
      "Epoch 1279 of 5000\n",
      "Epoch 1280 of 5000\n",
      "Epoch 1281 of 5000\n",
      "Epoch 1282 of 5000\n",
      "Epoch 1283 of 5000\n",
      "Epoch 1284 of 5000\n",
      "Epoch 1285 of 5000\n",
      "Epoch 1286 of 5000\n",
      "Epoch 1287 of 5000\n",
      "Epoch 1288 of 5000\n",
      "Epoch 1289 of 5000\n",
      "Epoch 1290 of 5000\n",
      "Epoch 1291 of 5000\n",
      "Epoch 1292 of 5000\n",
      "Epoch 1293 of 5000\n",
      "Epoch 1294 of 5000\n",
      "Epoch 1295 of 5000\n",
      "Epoch 1296 of 5000\n",
      "Epoch 1297 of 5000\n",
      "Epoch 1298 of 5000\n",
      "Epoch 1299 of 5000\n",
      "Epoch 1300 of 5000\n",
      "Epoch 1301 of 5000\n",
      "Epoch 1302 of 5000\n",
      "Epoch 1303 of 5000\n",
      "Epoch 1304 of 5000\n",
      "Epoch 1305 of 5000\n",
      "Epoch 1306 of 5000\n",
      "Epoch 1307 of 5000\n",
      "Epoch 1308 of 5000\n",
      "Epoch 1309 of 5000\n",
      "Epoch 1310 of 5000\n",
      "Epoch 1311 of 5000\n",
      "Epoch 1312 of 5000\n",
      "Epoch 1313 of 5000\n",
      "Epoch 1314 of 5000\n",
      "Epoch 1315 of 5000\n",
      "Epoch 1316 of 5000\n",
      "Epoch 1317 of 5000\n",
      "Epoch 1318 of 5000\n",
      "Epoch 1319 of 5000\n",
      "Epoch 1320 of 5000\n",
      "Epoch 1321 of 5000\n",
      "Epoch 1322 of 5000\n",
      "Epoch 1323 of 5000\n",
      "Epoch 1324 of 5000\n",
      "Epoch 1325 of 5000\n",
      "Epoch 1326 of 5000\n",
      "Epoch 1327 of 5000\n",
      "Epoch 1328 of 5000\n",
      "Epoch 1329 of 5000\n",
      "Epoch 1330 of 5000\n",
      "Epoch 1331 of 5000\n",
      "Epoch 1332 of 5000\n",
      "Epoch 1333 of 5000\n",
      "Epoch 1334 of 5000\n",
      "Epoch 1335 of 5000\n",
      "Epoch 1336 of 5000\n",
      "Epoch 1337 of 5000\n",
      "Epoch 1338 of 5000\n",
      "Epoch 1339 of 5000\n",
      "Epoch 1340 of 5000\n",
      "Epoch 1341 of 5000\n",
      "Epoch 1342 of 5000\n",
      "Epoch 1343 of 5000\n",
      "Epoch 1344 of 5000\n",
      "Epoch 1345 of 5000\n",
      "Epoch 1346 of 5000\n",
      "Epoch 1347 of 5000\n",
      "Epoch 1348 of 5000\n",
      "Epoch 1349 of 5000\n",
      "Epoch 1350 of 5000\n",
      "Epoch 1351 of 5000\n",
      "Epoch 1352 of 5000\n",
      "Epoch 1353 of 5000\n",
      "Epoch 1354 of 5000\n",
      "Epoch 1355 of 5000\n",
      "Epoch 1356 of 5000\n",
      "Epoch 1357 of 5000\n",
      "Epoch 1358 of 5000\n",
      "Epoch 1359 of 5000\n",
      "Epoch 1360 of 5000\n",
      "Epoch 1361 of 5000\n",
      "Epoch 1362 of 5000\n",
      "Epoch 1363 of 5000\n",
      "Epoch 1364 of 5000\n",
      "Epoch 1365 of 5000\n",
      "Epoch 1366 of 5000\n",
      "Epoch 1367 of 5000\n",
      "Epoch 1368 of 5000\n",
      "Epoch 1369 of 5000\n",
      "Epoch 1370 of 5000\n",
      "Epoch 1371 of 5000\n",
      "Epoch 1372 of 5000\n",
      "Epoch 1373 of 5000\n",
      "Epoch 1374 of 5000\n",
      "Epoch 1375 of 5000\n",
      "Epoch 1376 of 5000\n",
      "Epoch 1377 of 5000\n",
      "Epoch 1378 of 5000\n",
      "Epoch 1379 of 5000\n",
      "Epoch 1380 of 5000\n",
      "Epoch 1381 of 5000\n",
      "Epoch 1382 of 5000\n",
      "Epoch 1383 of 5000\n",
      "Epoch 1384 of 5000\n",
      "Epoch 1385 of 5000\n",
      "Epoch 1386 of 5000\n",
      "Epoch 1387 of 5000\n",
      "Epoch 1388 of 5000\n",
      "Epoch 1389 of 5000\n",
      "Epoch 1390 of 5000\n",
      "Epoch 1391 of 5000\n",
      "Epoch 1392 of 5000\n",
      "Epoch 1393 of 5000\n",
      "Epoch 1394 of 5000\n",
      "Epoch 1395 of 5000\n",
      "Epoch 1396 of 5000\n",
      "Epoch 1397 of 5000\n",
      "Epoch 1398 of 5000\n",
      "Epoch 1399 of 5000\n",
      "Epoch 1400 of 5000\n",
      "Epoch 1401 of 5000\n",
      "Epoch 1402 of 5000\n",
      "Epoch 1403 of 5000\n",
      "Epoch 1404 of 5000\n",
      "Epoch 1405 of 5000\n",
      "Epoch 1406 of 5000\n",
      "Epoch 1407 of 5000\n",
      "Epoch 1408 of 5000\n",
      "Epoch 1409 of 5000\n",
      "Epoch 1410 of 5000\n",
      "Epoch 1411 of 5000\n",
      "Epoch 1412 of 5000\n",
      "Epoch 1413 of 5000\n",
      "Epoch 1414 of 5000\n",
      "Epoch 1415 of 5000\n",
      "Epoch 1416 of 5000\n",
      "Epoch 1417 of 5000\n",
      "Epoch 1418 of 5000\n",
      "Epoch 1419 of 5000\n",
      "Epoch 1420 of 5000\n",
      "Epoch 1421 of 5000\n",
      "Epoch 1422 of 5000\n",
      "Epoch 1423 of 5000\n",
      "Epoch 1424 of 5000\n",
      "Epoch 1425 of 5000\n",
      "Epoch 1426 of 5000\n",
      "Epoch 1427 of 5000\n",
      "Epoch 1428 of 5000\n",
      "Epoch 1429 of 5000\n",
      "Epoch 1430 of 5000\n",
      "Epoch 1431 of 5000\n",
      "Epoch 1432 of 5000\n",
      "Epoch 1433 of 5000\n",
      "Epoch 1434 of 5000\n",
      "Epoch 1435 of 5000\n",
      "Epoch 1436 of 5000\n",
      "Epoch 1437 of 5000\n",
      "Epoch 1438 of 5000\n",
      "Epoch 1439 of 5000\n",
      "Epoch 1440 of 5000\n",
      "Epoch 1441 of 5000\n",
      "Epoch 1442 of 5000\n",
      "Epoch 1443 of 5000\n",
      "Epoch 1444 of 5000\n",
      "Epoch 1445 of 5000\n",
      "Epoch 1446 of 5000\n",
      "Epoch 1447 of 5000\n",
      "Epoch 1448 of 5000\n",
      "Epoch 1449 of 5000\n",
      "Epoch 1450 of 5000\n",
      "Epoch 1451 of 5000\n",
      "Epoch 1452 of 5000\n",
      "Epoch 1453 of 5000\n",
      "Epoch 1454 of 5000\n",
      "Epoch 1455 of 5000\n",
      "Epoch 1456 of 5000\n",
      "Epoch 1457 of 5000\n",
      "Epoch 1458 of 5000\n",
      "Epoch 1459 of 5000\n",
      "Epoch 1460 of 5000\n",
      "Epoch 1461 of 5000\n",
      "Epoch 1462 of 5000\n",
      "Epoch 1463 of 5000\n",
      "Epoch 1464 of 5000\n",
      "Epoch 1465 of 5000\n",
      "Epoch 1466 of 5000\n",
      "Epoch 1467 of 5000\n",
      "Epoch 1468 of 5000\n",
      "Epoch 1469 of 5000\n",
      "Epoch 1470 of 5000\n",
      "Epoch 1471 of 5000\n",
      "Epoch 1472 of 5000\n",
      "Epoch 1473 of 5000\n",
      "Epoch 1474 of 5000\n",
      "Epoch 1475 of 5000\n",
      "Epoch 1476 of 5000\n",
      "Epoch 1477 of 5000\n",
      "Epoch 1478 of 5000\n",
      "Epoch 1479 of 5000\n",
      "Epoch 1480 of 5000\n",
      "Epoch 1481 of 5000\n",
      "Epoch 1482 of 5000\n",
      "Epoch 1483 of 5000\n",
      "Epoch 1484 of 5000\n",
      "Epoch 1485 of 5000\n",
      "Epoch 1486 of 5000\n",
      "Epoch 1487 of 5000\n",
      "Epoch 1488 of 5000\n",
      "Epoch 1489 of 5000\n",
      "Epoch 1490 of 5000\n",
      "Epoch 1491 of 5000\n",
      "Epoch 1492 of 5000\n",
      "Epoch 1493 of 5000\n",
      "Epoch 1494 of 5000\n",
      "Epoch 1495 of 5000\n",
      "Epoch 1496 of 5000\n",
      "Epoch 1497 of 5000\n",
      "Epoch 1498 of 5000\n",
      "Epoch 1499 of 5000\n",
      "Epoch 1500 of 5000\n",
      "Epoch 1501 of 5000\n",
      "Epoch 1502 of 5000\n",
      "Epoch 1503 of 5000\n",
      "Epoch 1504 of 5000\n",
      "Epoch 1505 of 5000\n",
      "Epoch 1506 of 5000\n",
      "Epoch 1507 of 5000\n",
      "Epoch 1508 of 5000\n",
      "Epoch 1509 of 5000\n",
      "Epoch 1510 of 5000\n",
      "Epoch 1511 of 5000\n",
      "Epoch 1512 of 5000\n",
      "Epoch 1513 of 5000\n",
      "Epoch 1514 of 5000\n",
      "Epoch 1515 of 5000\n",
      "Epoch 1516 of 5000\n",
      "Epoch 1517 of 5000\n",
      "Epoch 1518 of 5000\n",
      "Epoch 1519 of 5000\n",
      "Epoch 1520 of 5000\n",
      "Epoch 1521 of 5000\n",
      "Epoch 1522 of 5000\n",
      "Epoch 1523 of 5000\n",
      "Epoch 1524 of 5000\n",
      "Epoch 1525 of 5000\n",
      "Epoch 1526 of 5000\n",
      "Epoch 1527 of 5000\n",
      "Epoch 1528 of 5000\n",
      "Epoch 1529 of 5000\n",
      "Epoch 1530 of 5000\n",
      "Epoch 1531 of 5000\n",
      "Epoch 1532 of 5000\n",
      "Epoch 1533 of 5000\n",
      "Epoch 1534 of 5000\n",
      "Epoch 1535 of 5000\n",
      "Epoch 1536 of 5000\n",
      "Epoch 1537 of 5000\n",
      "Epoch 1538 of 5000\n",
      "Epoch 1539 of 5000\n",
      "Epoch 1540 of 5000\n",
      "Epoch 1541 of 5000\n",
      "Epoch 1542 of 5000\n",
      "Epoch 1543 of 5000\n",
      "Epoch 1544 of 5000\n",
      "Epoch 1545 of 5000\n",
      "Epoch 1546 of 5000\n",
      "Epoch 1547 of 5000\n",
      "Epoch 1548 of 5000\n",
      "Epoch 1549 of 5000\n",
      "Epoch 1550 of 5000\n",
      "Epoch 1551 of 5000\n",
      "Epoch 1552 of 5000\n",
      "Epoch 1553 of 5000\n",
      "Epoch 1554 of 5000\n",
      "Epoch 1555 of 5000\n",
      "Epoch 1556 of 5000\n",
      "Epoch 1557 of 5000\n",
      "Epoch 1558 of 5000\n",
      "Epoch 1559 of 5000\n",
      "Epoch 1560 of 5000\n",
      "Epoch 1561 of 5000\n",
      "Epoch 1562 of 5000\n",
      "Epoch 1563 of 5000\n",
      "Epoch 1564 of 5000\n",
      "Epoch 1565 of 5000\n",
      "Epoch 1566 of 5000\n",
      "Epoch 1567 of 5000\n",
      "Epoch 1568 of 5000\n",
      "Epoch 1569 of 5000\n",
      "Epoch 1570 of 5000\n",
      "Epoch 1571 of 5000\n",
      "Epoch 1572 of 5000\n",
      "Epoch 1573 of 5000\n",
      "Epoch 1574 of 5000\n",
      "Epoch 1575 of 5000\n",
      "Epoch 1576 of 5000\n",
      "Epoch 1577 of 5000\n",
      "Epoch 1578 of 5000\n",
      "Epoch 1579 of 5000\n",
      "Epoch 1580 of 5000\n",
      "Epoch 1581 of 5000\n",
      "Epoch 1582 of 5000\n",
      "Epoch 1583 of 5000\n",
      "Epoch 1584 of 5000\n",
      "Epoch 1585 of 5000\n",
      "Epoch 1586 of 5000\n",
      "Epoch 1587 of 5000\n",
      "Epoch 1588 of 5000\n",
      "Epoch 1589 of 5000\n",
      "Epoch 1590 of 5000\n",
      "Epoch 1591 of 5000\n",
      "Epoch 1592 of 5000\n",
      "Epoch 1593 of 5000\n",
      "Epoch 1594 of 5000\n",
      "Epoch 1595 of 5000\n",
      "Epoch 1596 of 5000\n",
      "Epoch 1597 of 5000\n",
      "Epoch 1598 of 5000\n",
      "Epoch 1599 of 5000\n",
      "Epoch 1600 of 5000\n",
      "Epoch 1601 of 5000\n",
      "Epoch 1602 of 5000\n",
      "Epoch 1603 of 5000\n",
      "Epoch 1604 of 5000\n",
      "Epoch 1605 of 5000\n",
      "Epoch 1606 of 5000\n",
      "Epoch 1607 of 5000\n",
      "Epoch 1608 of 5000\n",
      "Epoch 1609 of 5000\n",
      "Epoch 1610 of 5000\n",
      "Epoch 1611 of 5000\n",
      "Epoch 1612 of 5000\n",
      "Epoch 1613 of 5000\n",
      "Epoch 1614 of 5000\n",
      "Epoch 1615 of 5000\n",
      "Epoch 1616 of 5000\n",
      "Epoch 1617 of 5000\n",
      "Epoch 1618 of 5000\n",
      "Epoch 1619 of 5000\n",
      "Epoch 1620 of 5000\n",
      "Epoch 1621 of 5000\n",
      "Epoch 1622 of 5000\n",
      "Epoch 1623 of 5000\n",
      "Epoch 1624 of 5000\n",
      "Epoch 1625 of 5000\n",
      "Epoch 1626 of 5000\n",
      "Epoch 1627 of 5000\n",
      "Epoch 1628 of 5000\n",
      "Epoch 1629 of 5000\n",
      "Epoch 1630 of 5000\n",
      "Epoch 1631 of 5000\n",
      "Epoch 1632 of 5000\n",
      "Epoch 1633 of 5000\n",
      "Epoch 1634 of 5000\n",
      "Epoch 1635 of 5000\n",
      "Epoch 1636 of 5000\n",
      "Epoch 1637 of 5000\n",
      "Epoch 1638 of 5000\n",
      "Epoch 1639 of 5000\n",
      "Epoch 1640 of 5000\n",
      "Epoch 1641 of 5000\n",
      "Epoch 1642 of 5000\n",
      "Epoch 1643 of 5000\n",
      "Epoch 1644 of 5000\n",
      "Epoch 1645 of 5000\n",
      "Epoch 1646 of 5000\n",
      "Epoch 1647 of 5000\n",
      "Epoch 1648 of 5000\n",
      "Epoch 1649 of 5000\n",
      "Epoch 1650 of 5000\n",
      "Epoch 1651 of 5000\n",
      "Epoch 1652 of 5000\n",
      "Epoch 1653 of 5000\n",
      "Epoch 1654 of 5000\n",
      "Epoch 1655 of 5000\n",
      "Epoch 1656 of 5000\n",
      "Epoch 1657 of 5000\n",
      "Epoch 1658 of 5000\n",
      "Epoch 1659 of 5000\n",
      "Epoch 1660 of 5000\n",
      "Epoch 1661 of 5000\n",
      "Epoch 1662 of 5000\n",
      "Epoch 1663 of 5000\n",
      "Epoch 1664 of 5000\n",
      "Epoch 1665 of 5000\n",
      "Epoch 1666 of 5000\n",
      "Epoch 1667 of 5000\n",
      "Epoch 1668 of 5000\n",
      "Epoch 1669 of 5000\n",
      "Epoch 1670 of 5000\n",
      "Epoch 1671 of 5000\n",
      "Epoch 1672 of 5000\n",
      "Epoch 1673 of 5000\n",
      "Epoch 1674 of 5000\n",
      "Epoch 1675 of 5000\n",
      "Epoch 1676 of 5000\n",
      "Epoch 1677 of 5000\n",
      "Epoch 1678 of 5000\n",
      "Epoch 1679 of 5000\n",
      "Epoch 1680 of 5000\n",
      "Epoch 1681 of 5000\n",
      "Epoch 1682 of 5000\n",
      "Epoch 1683 of 5000\n",
      "Epoch 1684 of 5000\n",
      "Epoch 1685 of 5000\n",
      "Epoch 1686 of 5000\n",
      "Epoch 1687 of 5000\n",
      "Epoch 1688 of 5000\n",
      "Epoch 1689 of 5000\n",
      "Epoch 1690 of 5000\n",
      "Epoch 1691 of 5000\n",
      "Epoch 1692 of 5000\n",
      "Epoch 1693 of 5000\n",
      "Epoch 1694 of 5000\n",
      "Epoch 1695 of 5000\n",
      "Epoch 1696 of 5000\n",
      "Epoch 1697 of 5000\n",
      "Epoch 1698 of 5000\n",
      "Epoch 1699 of 5000\n",
      "Epoch 1700 of 5000\n",
      "Epoch 1701 of 5000\n",
      "Epoch 1702 of 5000\n",
      "Epoch 1703 of 5000\n",
      "Epoch 1704 of 5000\n",
      "Epoch 1705 of 5000\n",
      "Epoch 1706 of 5000\n",
      "Epoch 1707 of 5000\n",
      "Epoch 1708 of 5000\n",
      "Epoch 1709 of 5000\n",
      "Epoch 1710 of 5000\n",
      "Epoch 1711 of 5000\n",
      "Epoch 1712 of 5000\n",
      "Epoch 1713 of 5000\n",
      "Epoch 1714 of 5000\n",
      "Epoch 1715 of 5000\n",
      "Epoch 1716 of 5000\n",
      "Epoch 1717 of 5000\n",
      "Epoch 1718 of 5000\n",
      "Epoch 1719 of 5000\n",
      "Epoch 1720 of 5000\n",
      "Epoch 1721 of 5000\n",
      "Epoch 1722 of 5000\n",
      "Epoch 1723 of 5000\n",
      "Epoch 1724 of 5000\n",
      "Epoch 1725 of 5000\n",
      "Epoch 1726 of 5000\n",
      "Epoch 1727 of 5000\n",
      "Epoch 1728 of 5000\n",
      "Epoch 1729 of 5000\n",
      "Epoch 1730 of 5000\n",
      "Epoch 1731 of 5000\n",
      "Epoch 1732 of 5000\n",
      "Epoch 1733 of 5000\n",
      "Epoch 1734 of 5000\n",
      "Epoch 1735 of 5000\n",
      "Epoch 1736 of 5000\n",
      "Epoch 1737 of 5000\n",
      "Epoch 1738 of 5000\n",
      "Epoch 1739 of 5000\n",
      "Epoch 1740 of 5000\n",
      "Epoch 1741 of 5000\n",
      "Epoch 1742 of 5000\n",
      "Epoch 1743 of 5000\n",
      "Epoch 1744 of 5000\n",
      "Epoch 1745 of 5000\n",
      "Epoch 1746 of 5000\n",
      "Epoch 1747 of 5000\n",
      "Epoch 1748 of 5000\n",
      "Epoch 1749 of 5000\n",
      "Epoch 1750 of 5000\n",
      "Epoch 1751 of 5000\n",
      "Epoch 1752 of 5000\n",
      "Epoch 1753 of 5000\n",
      "Epoch 1754 of 5000\n",
      "Epoch 1755 of 5000\n",
      "Epoch 1756 of 5000\n",
      "Epoch 1757 of 5000\n",
      "Epoch 1758 of 5000\n",
      "Epoch 1759 of 5000\n",
      "Epoch 1760 of 5000\n",
      "Epoch 1761 of 5000\n",
      "Epoch 1762 of 5000\n",
      "Epoch 1763 of 5000\n",
      "Epoch 1764 of 5000\n",
      "Epoch 1765 of 5000\n",
      "Epoch 1766 of 5000\n",
      "Epoch 1767 of 5000\n",
      "Epoch 1768 of 5000\n",
      "Epoch 1769 of 5000\n",
      "Epoch 1770 of 5000\n",
      "Epoch 1771 of 5000\n",
      "Epoch 1772 of 5000\n",
      "Epoch 1773 of 5000\n",
      "Epoch 1774 of 5000\n",
      "Epoch 1775 of 5000\n",
      "Epoch 1776 of 5000\n",
      "Epoch 1777 of 5000\n",
      "Epoch 1778 of 5000\n",
      "Epoch 1779 of 5000\n",
      "Epoch 1780 of 5000\n",
      "Epoch 1781 of 5000\n",
      "Epoch 1782 of 5000\n",
      "Epoch 1783 of 5000\n",
      "Epoch 1784 of 5000\n",
      "Epoch 1785 of 5000\n",
      "Epoch 1786 of 5000\n",
      "Epoch 1787 of 5000\n",
      "Epoch 1788 of 5000\n",
      "Epoch 1789 of 5000\n",
      "Epoch 1790 of 5000\n",
      "Epoch 1791 of 5000\n",
      "Epoch 1792 of 5000\n",
      "Epoch 1793 of 5000\n",
      "Epoch 1794 of 5000\n",
      "Epoch 1795 of 5000\n",
      "Epoch 1796 of 5000\n",
      "Epoch 1797 of 5000\n",
      "Epoch 1798 of 5000\n",
      "Epoch 1799 of 5000\n",
      "Epoch 1800 of 5000\n",
      "Epoch 1801 of 5000\n",
      "Epoch 1802 of 5000\n",
      "Epoch 1803 of 5000\n",
      "Epoch 1804 of 5000\n",
      "Epoch 1805 of 5000\n",
      "Epoch 1806 of 5000\n",
      "Epoch 1807 of 5000\n",
      "Epoch 1808 of 5000\n",
      "Epoch 1809 of 5000\n",
      "Epoch 1810 of 5000\n",
      "Epoch 1811 of 5000\n",
      "Epoch 1812 of 5000\n",
      "Epoch 1813 of 5000\n",
      "Epoch 1814 of 5000\n",
      "Epoch 1815 of 5000\n",
      "Epoch 1816 of 5000\n",
      "Epoch 1817 of 5000\n",
      "Epoch 1818 of 5000\n",
      "Epoch 1819 of 5000\n",
      "Epoch 1820 of 5000\n",
      "Epoch 1821 of 5000\n",
      "Epoch 1822 of 5000\n",
      "Epoch 1823 of 5000\n",
      "Epoch 1824 of 5000\n",
      "Epoch 1825 of 5000\n",
      "Epoch 1826 of 5000\n",
      "Epoch 1827 of 5000\n",
      "Epoch 1828 of 5000\n",
      "Epoch 1829 of 5000\n",
      "Epoch 1830 of 5000\n",
      "Epoch 1831 of 5000\n",
      "Epoch 1832 of 5000\n",
      "Epoch 1833 of 5000\n",
      "Epoch 1834 of 5000\n",
      "Epoch 1835 of 5000\n",
      "Epoch 1836 of 5000\n",
      "Epoch 1837 of 5000\n",
      "Epoch 1838 of 5000\n",
      "Epoch 1839 of 5000\n",
      "Epoch 1840 of 5000\n",
      "Epoch 1841 of 5000\n",
      "Epoch 1842 of 5000\n",
      "Epoch 1843 of 5000\n",
      "Epoch 1844 of 5000\n",
      "Epoch 1845 of 5000\n",
      "Epoch 1846 of 5000\n",
      "Epoch 1847 of 5000\n",
      "Epoch 1848 of 5000\n",
      "Epoch 1849 of 5000\n",
      "Epoch 1850 of 5000\n",
      "Epoch 1851 of 5000\n",
      "Epoch 1852 of 5000\n",
      "Epoch 1853 of 5000\n",
      "Epoch 1854 of 5000\n",
      "Epoch 1855 of 5000\n",
      "Epoch 1856 of 5000\n",
      "Epoch 1857 of 5000\n",
      "Epoch 1858 of 5000\n",
      "Epoch 1859 of 5000\n",
      "Epoch 1860 of 5000\n",
      "Epoch 1861 of 5000\n",
      "Epoch 1862 of 5000\n",
      "Epoch 1863 of 5000\n",
      "Epoch 1864 of 5000\n",
      "Epoch 1865 of 5000\n",
      "Epoch 1866 of 5000\n",
      "Epoch 1867 of 5000\n",
      "Epoch 1868 of 5000\n",
      "Epoch 1869 of 5000\n",
      "Epoch 1870 of 5000\n",
      "Epoch 1871 of 5000\n",
      "Epoch 1872 of 5000\n",
      "Epoch 1873 of 5000\n",
      "Epoch 1874 of 5000\n",
      "Epoch 1875 of 5000\n",
      "Epoch 1876 of 5000\n",
      "Epoch 1877 of 5000\n",
      "Epoch 1878 of 5000\n",
      "Epoch 1879 of 5000\n",
      "Epoch 1880 of 5000\n",
      "Epoch 1881 of 5000\n",
      "Epoch 1882 of 5000\n",
      "Epoch 1883 of 5000\n",
      "Epoch 1884 of 5000\n",
      "Epoch 1885 of 5000\n",
      "Epoch 1886 of 5000\n",
      "Epoch 1887 of 5000\n",
      "Epoch 1888 of 5000\n",
      "Epoch 1889 of 5000\n",
      "Epoch 1890 of 5000\n",
      "Epoch 1891 of 5000\n",
      "Epoch 1892 of 5000\n",
      "Epoch 1893 of 5000\n",
      "Epoch 1894 of 5000\n",
      "Epoch 1895 of 5000\n",
      "Epoch 1896 of 5000\n",
      "Epoch 1897 of 5000\n",
      "Epoch 1898 of 5000\n",
      "Epoch 1899 of 5000\n",
      "Epoch 1900 of 5000\n",
      "Epoch 1901 of 5000\n",
      "Epoch 1902 of 5000\n",
      "Epoch 1903 of 5000\n",
      "Epoch 1904 of 5000\n",
      "Epoch 1905 of 5000\n",
      "Epoch 1906 of 5000\n",
      "Epoch 1907 of 5000\n",
      "Epoch 1908 of 5000\n",
      "Epoch 1909 of 5000\n",
      "Epoch 1910 of 5000\n",
      "Epoch 1911 of 5000\n",
      "Epoch 1912 of 5000\n",
      "Epoch 1913 of 5000\n",
      "Epoch 1914 of 5000\n",
      "Epoch 1915 of 5000\n",
      "Epoch 1916 of 5000\n",
      "Epoch 1917 of 5000\n",
      "Epoch 1918 of 5000\n",
      "Epoch 1919 of 5000\n",
      "Epoch 1920 of 5000\n",
      "Epoch 1921 of 5000\n",
      "Epoch 1922 of 5000\n",
      "Epoch 1923 of 5000\n",
      "Epoch 1924 of 5000\n",
      "Epoch 1925 of 5000\n",
      "Epoch 1926 of 5000\n",
      "Epoch 1927 of 5000\n",
      "Epoch 1928 of 5000\n",
      "Epoch 1929 of 5000\n",
      "Epoch 1930 of 5000\n",
      "Epoch 1931 of 5000\n",
      "Epoch 1932 of 5000\n",
      "Epoch 1933 of 5000\n",
      "Epoch 1934 of 5000\n",
      "Epoch 1935 of 5000\n",
      "Epoch 1936 of 5000\n",
      "Epoch 1937 of 5000\n",
      "Epoch 1938 of 5000\n",
      "Epoch 1939 of 5000\n",
      "Epoch 1940 of 5000\n",
      "Epoch 1941 of 5000\n",
      "Epoch 1942 of 5000\n",
      "Epoch 1943 of 5000\n",
      "Epoch 1944 of 5000\n",
      "Epoch 1945 of 5000\n",
      "Epoch 1946 of 5000\n",
      "Epoch 1947 of 5000\n",
      "Epoch 1948 of 5000\n",
      "Epoch 1949 of 5000\n",
      "Epoch 1950 of 5000\n",
      "Epoch 1951 of 5000\n",
      "Epoch 1952 of 5000\n",
      "Epoch 1953 of 5000\n",
      "Epoch 1954 of 5000\n",
      "Epoch 1955 of 5000\n",
      "Epoch 1956 of 5000\n",
      "Epoch 1957 of 5000\n",
      "Epoch 1958 of 5000\n",
      "Epoch 1959 of 5000\n",
      "Epoch 1960 of 5000\n",
      "Epoch 1961 of 5000\n",
      "Epoch 1962 of 5000\n",
      "Epoch 1963 of 5000\n",
      "Epoch 1964 of 5000\n",
      "Epoch 1965 of 5000\n",
      "Epoch 1966 of 5000\n",
      "Epoch 1967 of 5000\n",
      "Epoch 1968 of 5000\n",
      "Epoch 1969 of 5000\n",
      "Epoch 1970 of 5000\n",
      "Epoch 1971 of 5000\n",
      "Epoch 1972 of 5000\n",
      "Epoch 1973 of 5000\n",
      "Epoch 1974 of 5000\n",
      "Epoch 1975 of 5000\n",
      "Epoch 1976 of 5000\n",
      "Epoch 1977 of 5000\n",
      "Epoch 1978 of 5000\n",
      "Epoch 1979 of 5000\n",
      "Epoch 1980 of 5000\n",
      "Epoch 1981 of 5000\n",
      "Epoch 1982 of 5000\n",
      "Epoch 1983 of 5000\n",
      "Epoch 1984 of 5000\n",
      "Epoch 1985 of 5000\n",
      "Epoch 1986 of 5000\n",
      "Epoch 1987 of 5000\n",
      "Epoch 1988 of 5000\n",
      "Epoch 1989 of 5000\n",
      "Epoch 1990 of 5000\n",
      "Epoch 1991 of 5000\n",
      "Epoch 1992 of 5000\n",
      "Epoch 1993 of 5000\n",
      "Epoch 1994 of 5000\n",
      "Epoch 1995 of 5000\n",
      "Epoch 1996 of 5000\n",
      "Epoch 1997 of 5000\n",
      "Epoch 1998 of 5000\n",
      "Epoch 1999 of 5000\n",
      "Epoch 2000 of 5000\n",
      "Epoch 2001 of 5000\n",
      "Epoch 2002 of 5000\n",
      "Epoch 2003 of 5000\n",
      "Epoch 2004 of 5000\n",
      "Epoch 2005 of 5000\n",
      "Epoch 2006 of 5000\n",
      "Epoch 2007 of 5000\n",
      "Epoch 2008 of 5000\n",
      "Epoch 2009 of 5000\n",
      "Epoch 2010 of 5000\n",
      "Epoch 2011 of 5000\n",
      "Epoch 2012 of 5000\n",
      "Epoch 2013 of 5000\n",
      "Epoch 2014 of 5000\n",
      "Epoch 2015 of 5000\n",
      "Epoch 2016 of 5000\n",
      "Epoch 2017 of 5000\n",
      "Epoch 2018 of 5000\n",
      "Epoch 2019 of 5000\n",
      "Epoch 2020 of 5000\n",
      "Epoch 2021 of 5000\n",
      "Epoch 2022 of 5000\n",
      "Epoch 2023 of 5000\n",
      "Epoch 2024 of 5000\n",
      "Epoch 2025 of 5000\n",
      "Epoch 2026 of 5000\n",
      "Epoch 2027 of 5000\n",
      "Epoch 2028 of 5000\n",
      "Epoch 2029 of 5000\n",
      "Epoch 2030 of 5000\n",
      "Epoch 2031 of 5000\n",
      "Epoch 2032 of 5000\n",
      "Epoch 2033 of 5000\n",
      "Epoch 2034 of 5000\n",
      "Epoch 2035 of 5000\n",
      "Epoch 2036 of 5000\n",
      "Epoch 2037 of 5000\n",
      "Epoch 2038 of 5000\n",
      "Epoch 2039 of 5000\n",
      "Epoch 2040 of 5000\n",
      "Epoch 2041 of 5000\n",
      "Epoch 2042 of 5000\n",
      "Epoch 2043 of 5000\n",
      "Epoch 2044 of 5000\n",
      "Epoch 2045 of 5000\n",
      "Epoch 2046 of 5000\n",
      "Epoch 2047 of 5000\n",
      "Epoch 2048 of 5000\n",
      "Epoch 2049 of 5000\n",
      "Epoch 2050 of 5000\n",
      "Epoch 2051 of 5000\n",
      "Epoch 2052 of 5000\n",
      "Epoch 2053 of 5000\n",
      "Epoch 2054 of 5000\n",
      "Epoch 2055 of 5000\n",
      "Epoch 2056 of 5000\n",
      "Epoch 2057 of 5000\n",
      "Epoch 2058 of 5000\n",
      "Epoch 2059 of 5000\n",
      "Epoch 2060 of 5000\n",
      "Epoch 2061 of 5000\n",
      "Epoch 2062 of 5000\n",
      "Epoch 2063 of 5000\n",
      "Epoch 2064 of 5000\n",
      "Epoch 2065 of 5000\n",
      "Epoch 2066 of 5000\n",
      "Epoch 2067 of 5000\n",
      "Epoch 2068 of 5000\n",
      "Epoch 2069 of 5000\n",
      "Epoch 2070 of 5000\n",
      "Epoch 2071 of 5000\n",
      "Epoch 2072 of 5000\n",
      "Epoch 2073 of 5000\n",
      "Epoch 2074 of 5000\n",
      "Epoch 2075 of 5000\n",
      "Epoch 2076 of 5000\n",
      "Epoch 2077 of 5000\n",
      "Epoch 2078 of 5000\n",
      "Epoch 2079 of 5000\n",
      "Epoch 2080 of 5000\n",
      "Epoch 2081 of 5000\n",
      "Epoch 2082 of 5000\n",
      "Epoch 2083 of 5000\n",
      "Epoch 2084 of 5000\n",
      "Epoch 2085 of 5000\n",
      "Epoch 2086 of 5000\n",
      "Epoch 2087 of 5000\n",
      "Epoch 2088 of 5000\n",
      "Epoch 2089 of 5000\n",
      "Epoch 2090 of 5000\n",
      "Epoch 2091 of 5000\n",
      "Epoch 2092 of 5000\n",
      "Epoch 2093 of 5000\n",
      "Epoch 2094 of 5000\n",
      "Epoch 2095 of 5000\n",
      "Epoch 2096 of 5000\n",
      "Epoch 2097 of 5000\n",
      "Epoch 2098 of 5000\n",
      "Epoch 2099 of 5000\n",
      "Epoch 2100 of 5000\n",
      "Epoch 2101 of 5000\n",
      "Epoch 2102 of 5000\n",
      "Epoch 2103 of 5000\n",
      "Epoch 2104 of 5000\n",
      "Epoch 2105 of 5000\n",
      "Epoch 2106 of 5000\n",
      "Epoch 2107 of 5000\n",
      "Epoch 2108 of 5000\n",
      "Epoch 2109 of 5000\n",
      "Epoch 2110 of 5000\n",
      "Epoch 2111 of 5000\n",
      "Epoch 2112 of 5000\n",
      "Epoch 2113 of 5000\n",
      "Epoch 2114 of 5000\n",
      "Epoch 2115 of 5000\n",
      "Epoch 2116 of 5000\n",
      "Epoch 2117 of 5000\n",
      "Epoch 2118 of 5000\n",
      "Epoch 2119 of 5000\n",
      "Epoch 2120 of 5000\n",
      "Epoch 2121 of 5000\n",
      "Epoch 2122 of 5000\n",
      "Epoch 2123 of 5000\n",
      "Epoch 2124 of 5000\n",
      "Epoch 2125 of 5000\n",
      "Epoch 2126 of 5000\n",
      "Epoch 2127 of 5000\n",
      "Epoch 2128 of 5000\n",
      "Epoch 2129 of 5000\n",
      "Epoch 2130 of 5000\n",
      "Epoch 2131 of 5000\n",
      "Epoch 2132 of 5000\n",
      "Epoch 2133 of 5000\n",
      "Epoch 2134 of 5000\n",
      "Epoch 2135 of 5000\n",
      "Epoch 2136 of 5000\n",
      "Epoch 2137 of 5000\n",
      "Epoch 2138 of 5000\n",
      "Epoch 2139 of 5000\n",
      "Epoch 2140 of 5000\n",
      "Epoch 2141 of 5000\n",
      "Epoch 2142 of 5000\n",
      "Epoch 2143 of 5000\n",
      "Epoch 2144 of 5000\n",
      "Epoch 2145 of 5000\n",
      "Epoch 2146 of 5000\n",
      "Epoch 2147 of 5000\n",
      "Epoch 2148 of 5000\n",
      "Epoch 2149 of 5000\n",
      "Epoch 2150 of 5000\n",
      "Epoch 2151 of 5000\n",
      "Epoch 2152 of 5000\n",
      "Epoch 2153 of 5000\n",
      "Epoch 2154 of 5000\n",
      "Epoch 2155 of 5000\n",
      "Epoch 2156 of 5000\n",
      "Epoch 2157 of 5000\n",
      "Epoch 2158 of 5000\n",
      "Epoch 2159 of 5000\n",
      "Epoch 2160 of 5000\n",
      "Epoch 2161 of 5000\n",
      "Epoch 2162 of 5000\n",
      "Epoch 2163 of 5000\n",
      "Epoch 2164 of 5000\n",
      "Epoch 2165 of 5000\n",
      "Epoch 2166 of 5000\n",
      "Epoch 2167 of 5000\n",
      "Epoch 2168 of 5000\n",
      "Epoch 2169 of 5000\n",
      "Epoch 2170 of 5000\n",
      "Epoch 2171 of 5000\n",
      "Epoch 2172 of 5000\n",
      "Epoch 2173 of 5000\n",
      "Epoch 2174 of 5000\n",
      "Epoch 2175 of 5000\n",
      "Epoch 2176 of 5000\n",
      "Epoch 2177 of 5000\n",
      "Epoch 2178 of 5000\n",
      "Epoch 2179 of 5000\n",
      "Epoch 2180 of 5000\n",
      "Epoch 2181 of 5000\n",
      "Epoch 2182 of 5000\n",
      "Epoch 2183 of 5000\n",
      "Epoch 2184 of 5000\n",
      "Epoch 2185 of 5000\n",
      "Epoch 2186 of 5000\n",
      "Epoch 2187 of 5000\n",
      "Epoch 2188 of 5000\n",
      "Epoch 2189 of 5000\n",
      "Epoch 2190 of 5000\n",
      "Epoch 2191 of 5000\n",
      "Epoch 2192 of 5000\n",
      "Epoch 2193 of 5000\n",
      "Epoch 2194 of 5000\n",
      "Epoch 2195 of 5000\n",
      "Epoch 2196 of 5000\n",
      "Epoch 2197 of 5000\n",
      "Epoch 2198 of 5000\n",
      "Epoch 2199 of 5000\n",
      "Epoch 2200 of 5000\n",
      "Epoch 2201 of 5000\n",
      "Epoch 2202 of 5000\n",
      "Epoch 2203 of 5000\n",
      "Epoch 2204 of 5000\n",
      "Epoch 2205 of 5000\n",
      "Epoch 2206 of 5000\n",
      "Epoch 2207 of 5000\n",
      "Epoch 2208 of 5000\n",
      "Epoch 2209 of 5000\n",
      "Epoch 2210 of 5000\n",
      "Epoch 2211 of 5000\n",
      "Epoch 2212 of 5000\n",
      "Epoch 2213 of 5000\n",
      "Epoch 2214 of 5000\n",
      "Epoch 2215 of 5000\n",
      "Epoch 2216 of 5000\n",
      "Epoch 2217 of 5000\n",
      "Epoch 2218 of 5000\n",
      "Epoch 2219 of 5000\n",
      "Epoch 2220 of 5000\n",
      "Epoch 2221 of 5000\n",
      "Epoch 2222 of 5000\n",
      "Epoch 2223 of 5000\n",
      "Epoch 2224 of 5000\n",
      "Epoch 2225 of 5000\n",
      "Epoch 2226 of 5000\n",
      "Epoch 2227 of 5000\n",
      "Epoch 2228 of 5000\n",
      "Epoch 2229 of 5000\n",
      "Epoch 2230 of 5000\n",
      "Epoch 2231 of 5000\n",
      "Epoch 2232 of 5000\n",
      "Epoch 2233 of 5000\n",
      "Epoch 2234 of 5000\n",
      "Epoch 2235 of 5000\n",
      "Epoch 2236 of 5000\n",
      "Epoch 2237 of 5000\n",
      "Epoch 2238 of 5000\n",
      "Epoch 2239 of 5000\n",
      "Epoch 2240 of 5000\n",
      "Epoch 2241 of 5000\n",
      "Epoch 2242 of 5000\n",
      "Epoch 2243 of 5000\n",
      "Epoch 2244 of 5000\n",
      "Epoch 2245 of 5000\n",
      "Epoch 2246 of 5000\n",
      "Epoch 2247 of 5000\n",
      "Epoch 2248 of 5000\n",
      "Epoch 2249 of 5000\n",
      "Epoch 2250 of 5000\n",
      "Epoch 2251 of 5000\n",
      "Epoch 2252 of 5000\n",
      "Epoch 2253 of 5000\n",
      "Epoch 2254 of 5000\n",
      "Epoch 2255 of 5000\n",
      "Epoch 2256 of 5000\n",
      "Epoch 2257 of 5000\n",
      "Epoch 2258 of 5000\n",
      "Epoch 2259 of 5000\n",
      "Epoch 2260 of 5000\n",
      "Epoch 2261 of 5000\n",
      "Epoch 2262 of 5000\n",
      "Epoch 2263 of 5000\n",
      "Epoch 2264 of 5000\n",
      "Epoch 2265 of 5000\n",
      "Epoch 2266 of 5000\n",
      "Epoch 2267 of 5000\n",
      "Epoch 2268 of 5000\n",
      "Epoch 2269 of 5000\n",
      "Epoch 2270 of 5000\n",
      "Epoch 2271 of 5000\n",
      "Epoch 2272 of 5000\n",
      "Epoch 2273 of 5000\n",
      "Epoch 2274 of 5000\n",
      "Epoch 2275 of 5000\n",
      "Epoch 2276 of 5000\n",
      "Epoch 2277 of 5000\n",
      "Epoch 2278 of 5000\n",
      "Epoch 2279 of 5000\n",
      "Epoch 2280 of 5000\n",
      "Epoch 2281 of 5000\n",
      "Epoch 2282 of 5000\n",
      "Epoch 2283 of 5000\n",
      "Epoch 2284 of 5000\n",
      "Epoch 2285 of 5000\n",
      "Epoch 2286 of 5000\n",
      "Epoch 2287 of 5000\n",
      "Epoch 2288 of 5000\n",
      "Epoch 2289 of 5000\n",
      "Epoch 2290 of 5000\n",
      "Epoch 2291 of 5000\n",
      "Epoch 2292 of 5000\n",
      "Epoch 2293 of 5000\n",
      "Epoch 2294 of 5000\n",
      "Epoch 2295 of 5000\n",
      "Epoch 2296 of 5000\n",
      "Epoch 2297 of 5000\n",
      "Epoch 2298 of 5000\n",
      "Epoch 2299 of 5000\n",
      "Epoch 2300 of 5000\n",
      "Epoch 2301 of 5000\n",
      "Epoch 2302 of 5000\n",
      "Epoch 2303 of 5000\n",
      "Epoch 2304 of 5000\n",
      "Epoch 2305 of 5000\n",
      "Epoch 2306 of 5000\n",
      "Epoch 2307 of 5000\n",
      "Epoch 2308 of 5000\n",
      "Epoch 2309 of 5000\n",
      "Epoch 2310 of 5000\n",
      "Epoch 2311 of 5000\n",
      "Epoch 2312 of 5000\n",
      "Epoch 2313 of 5000\n",
      "Epoch 2314 of 5000\n",
      "Epoch 2315 of 5000\n",
      "Epoch 2316 of 5000\n",
      "Epoch 2317 of 5000\n",
      "Epoch 2318 of 5000\n",
      "Epoch 2319 of 5000\n",
      "Epoch 2320 of 5000\n",
      "Epoch 2321 of 5000\n",
      "Epoch 2322 of 5000\n",
      "Epoch 2323 of 5000\n",
      "Epoch 2324 of 5000\n",
      "Epoch 2325 of 5000\n",
      "Epoch 2326 of 5000\n",
      "Epoch 2327 of 5000\n",
      "Epoch 2328 of 5000\n",
      "Epoch 2329 of 5000\n",
      "Epoch 2330 of 5000\n",
      "Epoch 2331 of 5000\n",
      "Epoch 2332 of 5000\n",
      "Epoch 2333 of 5000\n",
      "Epoch 2334 of 5000\n",
      "Epoch 2335 of 5000\n",
      "Epoch 2336 of 5000\n",
      "Epoch 2337 of 5000\n",
      "Epoch 2338 of 5000\n",
      "Epoch 2339 of 5000\n",
      "Epoch 2340 of 5000\n",
      "Epoch 2341 of 5000\n",
      "Epoch 2342 of 5000\n",
      "Epoch 2343 of 5000\n",
      "Epoch 2344 of 5000\n",
      "Epoch 2345 of 5000\n",
      "Epoch 2346 of 5000\n",
      "Epoch 2347 of 5000\n",
      "Epoch 2348 of 5000\n",
      "Epoch 2349 of 5000\n",
      "Epoch 2350 of 5000\n",
      "Epoch 2351 of 5000\n",
      "Epoch 2352 of 5000\n",
      "Epoch 2353 of 5000\n",
      "Epoch 2354 of 5000\n",
      "Epoch 2355 of 5000\n",
      "Epoch 2356 of 5000\n",
      "Epoch 2357 of 5000\n",
      "Epoch 2358 of 5000\n",
      "Epoch 2359 of 5000\n",
      "Epoch 2360 of 5000\n",
      "Epoch 2361 of 5000\n",
      "Epoch 2362 of 5000\n",
      "Epoch 2363 of 5000\n",
      "Epoch 2364 of 5000\n",
      "Epoch 2365 of 5000\n",
      "Epoch 2366 of 5000\n",
      "Epoch 2367 of 5000\n",
      "Epoch 2368 of 5000\n",
      "Epoch 2369 of 5000\n",
      "Epoch 2370 of 5000\n",
      "Epoch 2371 of 5000\n",
      "Epoch 2372 of 5000\n",
      "Epoch 2373 of 5000\n",
      "Epoch 2374 of 5000\n",
      "Epoch 2375 of 5000\n",
      "Epoch 2376 of 5000\n",
      "Epoch 2377 of 5000\n",
      "Epoch 2378 of 5000\n",
      "Epoch 2379 of 5000\n",
      "Epoch 2380 of 5000\n",
      "Epoch 2381 of 5000\n",
      "Epoch 2382 of 5000\n",
      "Epoch 2383 of 5000\n",
      "Epoch 2384 of 5000\n",
      "Epoch 2385 of 5000\n",
      "Epoch 2386 of 5000\n",
      "Epoch 2387 of 5000\n",
      "Epoch 2388 of 5000\n",
      "Epoch 2389 of 5000\n",
      "Epoch 2390 of 5000\n",
      "Epoch 2391 of 5000\n",
      "Epoch 2392 of 5000\n",
      "Epoch 2393 of 5000\n",
      "Epoch 2394 of 5000\n",
      "Epoch 2395 of 5000\n",
      "Epoch 2396 of 5000\n",
      "Epoch 2397 of 5000\n",
      "Epoch 2398 of 5000\n",
      "Epoch 2399 of 5000\n",
      "Epoch 2400 of 5000\n",
      "Epoch 2401 of 5000\n",
      "Epoch 2402 of 5000\n",
      "Epoch 2403 of 5000\n",
      "Epoch 2404 of 5000\n",
      "Epoch 2405 of 5000\n",
      "Epoch 2406 of 5000\n",
      "Epoch 2407 of 5000\n",
      "Epoch 2408 of 5000\n",
      "Epoch 2409 of 5000\n",
      "Epoch 2410 of 5000\n",
      "Epoch 2411 of 5000\n",
      "Epoch 2412 of 5000\n",
      "Epoch 2413 of 5000\n",
      "Epoch 2414 of 5000\n",
      "Epoch 2415 of 5000\n",
      "Epoch 2416 of 5000\n",
      "Epoch 2417 of 5000\n",
      "Epoch 2418 of 5000\n",
      "Epoch 2419 of 5000\n",
      "Epoch 2420 of 5000\n",
      "Epoch 2421 of 5000\n",
      "Epoch 2422 of 5000\n",
      "Epoch 2423 of 5000\n",
      "Epoch 2424 of 5000\n",
      "Epoch 2425 of 5000\n",
      "Epoch 2426 of 5000\n",
      "Epoch 2427 of 5000\n",
      "Epoch 2428 of 5000\n",
      "Epoch 2429 of 5000\n",
      "Epoch 2430 of 5000\n",
      "Epoch 2431 of 5000\n",
      "Epoch 2432 of 5000\n",
      "Epoch 2433 of 5000\n",
      "Epoch 2434 of 5000\n",
      "Epoch 2435 of 5000\n",
      "Epoch 2436 of 5000\n",
      "Epoch 2437 of 5000\n",
      "Epoch 2438 of 5000\n",
      "Epoch 2439 of 5000\n",
      "Epoch 2440 of 5000\n",
      "Epoch 2441 of 5000\n",
      "Epoch 2442 of 5000\n",
      "Epoch 2443 of 5000\n",
      "Epoch 2444 of 5000\n",
      "Epoch 2445 of 5000\n",
      "Epoch 2446 of 5000\n",
      "Epoch 2447 of 5000\n",
      "Epoch 2448 of 5000\n",
      "Epoch 2449 of 5000\n",
      "Epoch 2450 of 5000\n",
      "Epoch 2451 of 5000\n",
      "Epoch 2452 of 5000\n",
      "Epoch 2453 of 5000\n",
      "Epoch 2454 of 5000\n",
      "Epoch 2455 of 5000\n",
      "Epoch 2456 of 5000\n",
      "Epoch 2457 of 5000\n",
      "Epoch 2458 of 5000\n",
      "Epoch 2459 of 5000\n",
      "Epoch 2460 of 5000\n",
      "Epoch 2461 of 5000\n",
      "Epoch 2462 of 5000\n",
      "Epoch 2463 of 5000\n",
      "Epoch 2464 of 5000\n",
      "Epoch 2465 of 5000\n",
      "Epoch 2466 of 5000\n",
      "Epoch 2467 of 5000\n",
      "Epoch 2468 of 5000\n",
      "Epoch 2469 of 5000\n",
      "Epoch 2470 of 5000\n",
      "Epoch 2471 of 5000\n",
      "Epoch 2472 of 5000\n",
      "Converged at epoch 2471\n",
      "Final training cost: 162920.40409297362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(162920.40409297362)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_price_prediction = neural_network.predict(test_id,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 2)\n"
     ]
    }
   ],
   "source": [
    "print(housing_price_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'submission.csv'\n",
    "housing_price_prediction.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
